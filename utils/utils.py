import torch
import torch.nn.functional as F
import numpy as np

import matplotlib.pyplot as plt

import math

# å…¨å±€ç¼“å­˜å˜é‡ï¼ˆåˆå§‹ä¸º Noneï¼‰
_GLOBAL_DIST_MATRIX_64 = None

def get_distance_matrix_64(device='cpu'):
    """
    è¿”å› 64x64 å›¾åƒçš„å…¨å±€åƒç´ è·ç¦»çŸ©é˜µ (4096, 4096)
    é¦–æ¬¡è°ƒç”¨æ—¶è®¡ç®—å¹¶ç¼“å­˜ï¼Œåç»­ç›´æ¥è¿”å›ï¼ˆè‡ªåŠ¨è¿ç§»åˆ°æŒ‡å®š deviceï¼‰
    """
    global _GLOBAL_DIST_MATRIX_64

    if _GLOBAL_DIST_MATRIX_64 is None:
        # åªåœ¨ CPU ä¸Šè®¡ç®—ä¸€æ¬¡ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰ï¼Œåç»­æŒ‰éœ€è¿ç§»åˆ° GPU
        # print("é¦–æ¬¡è®¡ç®— 64x64 è·ç¦»çŸ©é˜µ...")
        H, W = 64, 64
        y = torch.arange(H, dtype=torch.float32)
        x = torch.arange(W, dtype=torch.float32)
        y_flat = y.repeat_interleave(W)  # (4096,)
        x_flat = x.repeat(H)             # (4096,)
        y_col = y_flat[:, None]  # (4096, 1)
        x_col = x_flat[:, None]
        y_row = y_flat[None, :]  # (1, 4096)
        x_row = x_flat[None, :]
        dist = torch.sqrt((y_col - y_row) ** 2 + (x_col - x_row) ** 2)
        _GLOBAL_DIST_MATRIX_64 = dist  # ä¿å­˜åœ¨ CPU ä¸Š

    return _GLOBAL_DIST_MATRIX_64.to(device)


def dilate_mask(mask, d=2):
    kernel_size = 2 * d + 1
    weight = torch.ones(1, 1, kernel_size, kernel_size, device=mask.device)
    mask = mask.unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]
    mask = mask.float()
    dilated = F.conv2d(mask, weight, padding=kernel_size // 2)
    dilated = (dilated > 0).float().squeeze()
    return dilated

def erode_mask(mask, d=2):
    kernel_size = 2 * d + 1
    weight = torch.ones(1, 1, kernel_size, kernel_size, device=mask.device)
    mask = mask.unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]
    # è…èš€ç­‰ä»·äºå·ç§¯ååˆ¤æ–­æ˜¯å¦ç­‰äº kernel ä¸­å…ƒç´ æ•°é‡
    eroded = F.conv2d(mask.float(), weight, padding=kernel_size // 2)
    eroded = (eroded == kernel_size * kernel_size).float().squeeze()
    return eroded

def iou_score(pred, target):
    smooth = 1e-11
    intersection = pred * target

    intersection_sum = np.sum(intersection, axis=(0,1))
    pred_sum = np.sum(pred, axis=(0,1))
    target_sum = np.sum(target, axis=(0,1))
    score = (intersection_sum) / (pred_sum + target_sum - intersection_sum + smooth)

    score = np.mean(score)
    return score

def gaussian_kernel(size, sigma, kernel_dim=1): 
    coords = torch.arange(size, dtype=torch.float32)
    coords -= (size - 1) / 2
    kernel_1d = torch.exp(-(coords ** 2) / (2 * sigma ** 2))
    kernel_1d /= kernel_1d.sum()

    kernel_ = []
    for i in range(kernel_dim):  # åˆ›å»ºä¸€ä¸ª [kernel_dim, size, size] çš„å¼ é‡
        kernel_.append(kernel_1d)

    kernel = torch.outer(*(kernel_)) if kernel_dim == 2 else kernel_1d
    kernel = kernel.unsqueeze(0).unsqueeze(0)
    return kernel

def gaussian_blurring_2D(tensor, kernel_size, sigma):
    if len(tensor.shape) == 2:
        tensor = tensor.unsqueeze(0).unsqueeze(0)
    kernel = gaussian_kernel(kernel_size, sigma, 2)
    result_mask = F.conv2d(tensor.float(), kernel, padding=kernel_size//2)
    return result_mask.squeeze(0).squeeze(0)

def extract_local_windows(tensor, window_size=5):
    """
    è¾“å…¥ tensor shape: (H, W)
    è¾“å‡º windows shape: (H, W, window_size, window_size)
    """
    H, W = tensor.shape
    pad = window_size // 2
    tensor = tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]
    tensor_paded = F.pad(tensor, (pad, pad, pad, pad), mode="replicate")
    # ä½¿ç”¨ unfold æå–å±€éƒ¨å—
    patches = F.unfold(tensor_paded, kernel_size=window_size)
    # reshape æˆçª—å£å½¢å¼
    _, c_kw2, n_patches = patches.shape
    patches = patches.transpose(1, 2).view(H, W, window_size, window_size)
    return patches

def mask_diameter(mask: torch.Tensor):
    """
    è¾“å…¥ä¸€ä¸ªäºŒå€¼ mask (shape [H, W])ï¼Œè¿”å›å…¶åŒºåŸŸå†…æœ€è¿œä¸¤ç‚¹é—´çš„æ¬§æ°è·ç¦»ï¼ˆå³ç›´å¾„ï¼‰
    
    Args:
        mask (Tensor): shape [H, W]ï¼Œå…¶ä¸­ 1 è¡¨ç¤ºç›®æ ‡åŒºåŸŸï¼Œ0 è¡¨ç¤ºèƒŒæ™¯
    
    Returns:
        Tensor: ç›´å¾„é•¿åº¦ï¼ˆæ ‡é‡ï¼‰
    """
    assert mask.dim() == 2, "mask å¿…é¡»æ˜¯äºŒç»´å¼ é‡"
    
    # è·å–è®¾å¤‡ä¿¡æ¯
    device = mask.device
    
    # è·å– mask ä¸­éé›¶ç‚¹çš„åæ ‡
    coords = torch.nonzero(mask)  # shape [N, 2]
    
    if coords.shape[0] < 2:
        return torch.tensor(0.0, device=device)

    # è®¡ç®—æ‰€æœ‰ç‚¹ä¹‹é—´çš„ä¸¤ä¸¤è·ç¦»
    diff = coords.unsqueeze(1) - coords.unsqueeze(0)  # [N, N, 2]
    diff = diff.float()
    dists = torch.hypot(diff[..., 0], diff[..., 1])   # æ¬§å¼è·ç¦»

    # å–æœ€å¤§è·ç¦»ä½œä¸ºç›´å¾„
    diameter = dists.max()

    return diameter

def compute_weighted_centroids(logits: torch.Tensor, edge_mask: torch.Tensor, topk_ratio=0.3):
    """
    è®¡ç®—é«˜/ä½ç½®ä¿¡åº¦åŒºåŸŸçš„åŠ æƒè´¨å¿ƒï¼Œè€ƒè™‘è¾¹ç¼˜é™„è¿‘åƒç´ çš„æƒé‡ã€‚
    
    Args:
        logits (Tensor): shape [H, W] æˆ– [C, H, W]
        edge_mask (Tensor): shape [H, W], 1 è¡¨ç¤ºè¾¹ç¼˜åŒºåŸŸ
        topk_ratio (float): å–å‰å¤šå°‘æ¯”ä¾‹çš„åƒç´ ä½œä¸ºé«˜/ä½ç½®ä¿¡åº¦åŒºåŸŸ
    
    Returns:
        Tuple[Tensor, Tensor]: é«˜ç½®ä¿¡åº¦è´¨å¿ƒ (y_high, x_high), ä½ç½®ä¿¡åº¦è´¨å¿ƒ (y_low, x_low)
    """
    assert edge_mask.shape == logits.shape[-2:], "edge_mask å’Œ logits çš„ç©ºé—´å°ºå¯¸å¿…é¡»ä¸€è‡´"

    if logits.dim() == 3:
        logits = logits[0]  # å¦‚æœæ˜¯å¤šç±»ï¼Œåªå–ç¬¬ä¸€ä¸ªç±»ä¸ºä¾‹ï¼Œä½ å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹

    H, W = logits.shape
    device = logits.device

    # åˆ›å»ºåæ ‡ç½‘æ ¼
    y_grid, x_grid = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing='ij')

    # è¾¹ç¼˜æƒé‡ï¼š1 + è¾¹ç¼˜é™„è¿‘å¢å¼ºç³»æ•°
    edge_mask_ = gaussian_blurring_2D(edge_mask, kernel_size=5, sigma=2.0)
    edge_weight = 1.0 + edge_mask_.float() * edge_mask

    # å½’ä¸€åŒ– logitsï¼ˆå¯é€‰ï¼‰
    logits_norm = (logits - logits.min()) / (logits.max() - logits.min() + 1e-8)

    # å– top-k åƒç´ ä½œä¸ºé«˜ç½®ä¿¡åº¦åŒºåŸŸ
    num_pixels = H * W
    topk_num = max(1, int(num_pixels * topk_ratio))

    high_flat = logits_norm.view(-1)
    _, high_indices = torch.topk(high_flat, topk_num)
    high_mask = torch.zeros_like(high_flat).scatter_(0, high_indices, 1).reshape(H, W).bool()

    low_flat = (1 - logits_norm).view(-1)
    _, low_indices = torch.topk(low_flat, topk_num)
    low_mask = torch.zeros_like(low_flat).scatter_(0, low_indices, 1).reshape(H, W).bool()

    # è®¡ç®—åŠ æƒè´¨å¿ƒ
    def weighted_centroid(mask, weight_map):
        weights = weight_map[mask]
        y_coords = y_grid[mask]
        x_coords = x_grid[mask]
        total_weight = weights.sum()
        cy = (y_coords * weights).sum() / total_weight.clamp(min=1e-8)
        cx = (x_coords * weights).sum() / total_weight.clamp(min=1e-8)
        return cy, cx

    centroid_high = weighted_centroid(high_mask, edge_weight)
    centroid_low = weighted_centroid(low_mask, edge_weight)

    return centroid_high, centroid_low

def farthest_point_sampling(mask, n_points):
    """
    åœ¨ mask ä¸­é€‰æ‹© n_points ä¸ªäºŒç»´ç©ºé—´ä¸Šæœ€è¿œçš„ç‚¹ã€‚
    
    å‚æ•°:
        mask: 2D numpy array (H, W), dtype=bool æˆ– dtype=uint8 (0 or 1)
        n_points: éœ€è¦é€‰æ‹©çš„ç‚¹æ•°
    
    è¿”å›:
        points: (n_points, 2) çš„æ•°ç»„ï¼Œæ¯ä¸ªç‚¹æ˜¯ (y, x) æˆ– (row, col)
    """
    if mask.dtype != bool:
        mask = mask.astype(bool)

    coords = np.argwhere(mask)

    if len(coords) == 0:
        raise ValueError("mask ä¸­æ²¡æœ‰å‰æ™¯åƒç´ ï¼ˆå€¼ä¸º1çš„ç‚¹ï¼‰ã€‚")

    if n_points > len(coords):
        raise ValueError(f"mask ä¸­åªæœ‰ {len(coords)} ä¸ªç‚¹ï¼Œæ— æ³•é€‰å‡º {n_points} ä¸ªç‚¹ã€‚")

    selected_indices = []
    first_idx = np.random.randint(len(coords))
    selected_indices.append(first_idx)

    # åˆå§‹åŒ–æ¯ä¸ªç‚¹åˆ°å½“å‰é€‰ä¸­ç‚¹çš„æœ€å°è·ç¦»
    min_distances = np.full(len(coords), np.inf)

    while len(selected_indices) < n_points:
        last_point = coords[selected_indices[-1]]
        # æ›´æ–°æ‰€æœ‰ç‚¹åˆ°æœ€æ–°é€‰ä¸­ç‚¹çš„è·ç¦»
        dists = np.linalg.norm(coords - last_point, axis=1)
        min_distances = np.minimum(min_distances, dists)
        
        # å¿½ç•¥å·²ç»é€‰è¿‡çš„ç‚¹
        min_distances[selected_indices] = -np.inf
        
        # æ‰¾å‡ºå½“å‰æœ€è¿œç‚¹ï¼ˆå³ min_distances æœ€å¤§çš„é‚£ä¸ªï¼‰
        farthest_idx = np.argmax(min_distances)
        selected_indices.append(farthest_idx)

    return coords[selected_indices]


def split_indices_by_mod(start, end, n, m):
    """
    ç”ŸæˆæŒ‡å®šèŒƒå›´å†…æ»¡è¶³æ¨¡ n ä½™ m çš„ç´¢å¼•åˆ—è¡¨ï¼ŒåŠå…¶è¡¥é›†ã€‚

    å‚æ•°:
        start (int): èµ·å§‹æ•°å­—ï¼ˆåŒ…å«ï¼‰
        end (int): ç»“æŸæ•°å­—ï¼ˆåŒ…å«ï¼‰
        n (int): æ¨¡æ•°ï¼Œå¿…é¡»å¤§äº0
        m (int): ä½™æ•°ï¼Œåº”æ»¡è¶³ 0 <= m < n

    è¿”å›:
        tuple: (mod_list, complement_list)
               mod_list: èŒƒå›´å†…æ»¡è¶³ i % n == m çš„æ•°å­—
               complement_list: èŒƒå›´å†…å…¶ä½™çš„æ•°å­—ï¼ˆè¡¥é›†ï¼‰
    """
    if n <= 0:
        raise ValueError("n å¿…é¡»å¤§äº 0")
    if not (0 <= m < n):
        raise ValueError(f"m å¿…é¡»æ»¡è¶³ 0 <= m < nï¼Œå½“å‰ m={m}, n={n}")

    full_range = range(start, end + 1)
    mod_list = [i for i in full_range if i % n == m]
    complement_list = [i for i in full_range if i % n != m]

    return mod_list, complement_list

def compute_mask_pixel_distances_with_coords(mask):
    """
    è®¡ç®— mask ä¸­æ‰€æœ‰å€¼ä¸º 1 çš„åƒç´ ä¹‹é—´çš„è·ç¦»ï¼Œå¹¶è¿”å›åæ ‡å’Œè·ç¦»çŸ©é˜µã€‚

    Args:
        mask (torch.Tensor): shape [H, W], dtype: int or bool

    Returns:
        coords (torch.Tensor): shape [n, 2], each row is (h, w)
        distances (torch.Tensor): shape [n, n], æ¯è¡Œæ˜¯è¯¥ç‚¹åˆ°å…¶ä»–ç‚¹çš„è·ç¦»
    """
    # æå–å€¼ä¸º1çš„åƒç´ åæ ‡
    coords = torch.nonzero(mask)  # shape [n, 2], (h, w)
    n = coords.shape[0]

    if n == 0:
        return coords, torch.empty(0, 0)
    if n == 1:
        return coords, torch.empty(1, 0)

    # è®¡ç®—è·ç¦»çŸ©é˜µ [n, n]
    diff = coords.unsqueeze(1) - coords.unsqueeze(0)  # [n, n, 2]
    dists = torch.sqrt(torch.sum(diff ** 2, dim=-1))  # æ¬§æ°è·ç¦» [n, n]

    # # å»æ‰å¯¹è§’çº¿ï¼Œå˜æˆ [n, n-1]
    # eye = torch.eye(n, dtype=torch.bool, device=mask.device)
    # distances = dists[~eye].view(n, n - 1)
    
    # å½’ä¸€åŒ–ï¼Œä½¿å¾—è·ç¦»åœ¨0-1ä¹‹é—´
    dists = (dists - dists.min()) / (dists.max() - dists.min())

    return coords, dists


def min_positive_per_local_area(tensor, default=0.0):
    """
    åœ¨æ¯ä¸ª local_area ä¸­æ‰¾åˆ°å¤§äº 0 çš„æœ€å°å€¼ã€‚

    Args:
        tensor: shape [H, W, k, k]
        default: å½“æ²¡æœ‰å¤§äº 0 çš„å€¼æ—¶ï¼Œè¿”å›çš„é»˜è®¤å€¼

    Returns:
        result: shape [H, W], æ¯ä¸ªä½ç½®æ˜¯å¯¹åº”åŒºåŸŸä¸­ >0 çš„æœ€å°å€¼
    """
    H, W, k1, k2 = tensor.shape
    flat = tensor.view(H, W, -1)  # [H, W, k1*k2]
    mask = flat > 0
    # å°†éæ­£æ•°æ›¿æ¢ä¸º infï¼Œé¿å…å½±å“ min
    inf_tensor = torch.full_like(flat, float('inf'))
    valid_values = torch.where(mask, flat, inf_tensor)
    min_vals = valid_values.min(dim=-1).values  # [H, W]
    # æ›¿æ¢ inf ä¸ºé»˜è®¤å€¼
    result = torch.where(torch.isinf(min_vals), torch.tensor(default, dtype=min_vals.dtype, device=min_vals.device), min_vals)
    return result


def compute_local_extremes(image, mask, mode='max', local_size=3):
    """
    è®¡ç®—maskåŒºåŸŸå†…åƒç´ çš„å±€éƒ¨æå€¼ï¼ˆæœ€å¤§å€¼æˆ–æœ€å°å€¼ï¼‰
    
    Args:
        image: å½¢çŠ¶ä¸º[H, W]çš„tensorï¼Œè¾“å…¥å›¾åƒ
        mask: å½¢çŠ¶ä¸º[H, W]çš„tensorï¼ŒäºŒå€¼æ©ç ï¼ŒTrue/1è¡¨ç¤ºæ„Ÿå…´è¶£åŒºåŸŸ
        mode: å­—ç¬¦ä¸²ï¼Œ'max'æˆ–'min'ï¼Œå†³å®šè®¡ç®—æœ€å¤§å€¼è¿˜æ˜¯æœ€å°å€¼
        local_size: æ•´æ•°ï¼Œpoolingçª—å£å¤§å°ï¼Œå¿…é¡»ä¸ºå¥‡æ•°
    
    Returns:
        local_extremes: å½¢çŠ¶ä¸º[H, W]çš„tensorï¼Œæ¯ä¸ªåƒç´ ä½ç½®çš„å±€éƒ¨æå€¼
    """
    # assert image.dim() == 2, "image should be 2D tensor with shape [H, W]"
    # assert mask.dim() == 2, "mask should be 2D tensor with shape [H, W]"
    # assert image.shape == mask.shape, "image and mask should have same shape"
    # assert mode in ['max', 'min'], "mode should be 'max' or 'min'"
    # assert local_size % 2 == 1, "local_size should be odd number"
    
    H, W = image.shape
    padding = local_size // 2
    
    # å°†imageå’Œmaskæ‰©å±•ä¸ºé€‚åˆå·ç§¯æ“ä½œçš„å½¢çŠ¶ [1, 1, H, W]
    image_expanded = image.unsqueeze(0).unsqueeze(0)
    mask_expanded = mask.unsqueeze(0).unsqueeze(0).float()
    
    # åˆ›å»ºç”¨äºæ ‡è®°æœ‰æ•ˆåƒç´ çš„æ‰©å±•mask
    # å¯¹maskè¿›è¡Œsame paddingçš„å·ç§¯ï¼Œç»Ÿè®¡æ¯ä¸ªçª—å£å†…æœ‰æ•ˆåƒç´ æ•°é‡
    local_extremes = torch.zeros_like(image)
    
    # å¯¹æ¯ä¸ªåƒç´ ä½ç½®è®¡ç®—å±€éƒ¨æå€¼
    for i in range(H):
        for j in range(W):
            # å¦‚æœå½“å‰åƒç´ ä¸åœ¨maskåŒºåŸŸå†…ï¼Œè·³è¿‡
            if not mask[i, j]:
                continue
                
            # è®¡ç®—å±€éƒ¨çª—å£çš„è¾¹ç•Œ
            i_start = max(0, i - padding)
            i_end = min(H, i + padding + 1)
            j_start = max(0, j - padding)
            j_end = min(W, j + padding + 1)
            
            # æå–å±€éƒ¨çª—å£
            local_image = image[i_start:i_end, j_start:j_end]
            local_mask = mask[i_start:i_end, j_start:j_end]
            
            # åªè€ƒè™‘maskä¸ºTrueçš„åƒç´ 
            valid_pixels = local_image[local_mask]
            
            # å¦‚æœçª—å£å†…æ²¡æœ‰æœ‰æ•ˆåƒç´ ï¼Œä½¿ç”¨åŸåƒç´ å€¼
            if valid_pixels.numel() == 0:
                local_extremes[i, j] = image[i, j]
            else:
                if mode == 'max':
                    local_extremes[i, j] = valid_pixels.max()
                else:  # mode == 'min'
                    local_extremes[i, j] = valid_pixels.min()
    
    return local_extremes


# def compute_weighted_variance(logits: torch.Tensor, mask: torch.Tensor):
    """
    è®¡ç®—æ¯ä¸ªåƒç´ ä½ç½®åœ¨ mask åŒºåŸŸå†…åŸºäºè·ç¦»åŠ æƒçš„æ–¹å·®ï¼ˆä¸¤ç§æƒ…å†µï¼‰ï¼š
    1. ä¸åŒ…å«å½“å‰åƒç´ çš„åŠ æƒæ–¹å·®
    2. åŒ…å«å½“å‰åƒç´ çš„åŠ æƒæ–¹å·®

    Args:
        logits: (H, W) çš„ Tensor
        mask: (H, W) çš„äºŒå€¼ Tensor (0 or 1)

    Returns:
        var_wo: (H, W) ä¸åŒ…å«å½“å‰åƒç´ çš„åŠ æƒæ–¹å·®
        var_w:  (H, W) åŒ…å«å½“å‰åƒç´ çš„åŠ æƒæ–¹å·®
    """
    assert logits.shape == mask.shape, "logits and mask must have the same shape"
    H, W = logits.shape

    # é¢„è®¡ç®—æ‰€æœ‰åƒç´ åæ ‡
    y_coords, x_coords = torch.meshgrid(torch.arange(H, device=logits.device),
                                        torch.arange(W, device=logits.device),
                                        indexing='ij')  # (H, W)

    # æœ€å¤§å¯èƒ½æ¬§æ°è·ç¦»ï¼ˆä»å·¦ä¸Šåˆ°å³ä¸‹ï¼‰
    max_dist = math.sqrt((H - 1)**2 + (W - 1)**2)

    # æ‰©å±•åæ ‡ä¸º (H, W, 1, 1) ä¾¿äºåç»­å¹¿æ’­
    y_coords = y_coords.unsqueeze(-1).unsqueeze(-1)  # (H, W, 1, 1)
    x_coords = x_coords.unsqueeze(-1).unsqueeze(-1)

    # è·å– mask ä¸­ä¸º 1 çš„æ‰€æœ‰ä½ç½®çš„åæ ‡
    mask_positions = mask.nonzero(as_tuple=False)  # (N, 2), each row is (i, j)
    N = mask_positions.shape[0]
    if N == 0:
        # å¦‚æœ mask å…¨ä¸º 0ï¼Œè¿”å›å…¨ 0 æ–¹å·®
        var_wo = torch.zeros_like(logits)
        var_w = torch.zeros_like(logits)
        return var_wo, var_w

    # æå– mask åŒºåŸŸå†…çš„ logits å€¼
    mask_logits_vals = logits[mask_positions[:, 0], mask_positions[:, 1]]  # (N,)

    # æ„é€  mask åŒºåŸŸçš„åæ ‡å¼ é‡ (N, 2) -> (1, 1, N, 2)
    mask_y = mask_positions[:, 0].view(1, 1, -1, 1)  # (1, 1, N, 1)
    mask_x = mask_positions[:, 1].view(1, 1, -1, 1)

    # è®¡ç®—æ¯ä¸ªåƒç´  (i,j) åˆ°æ‰€æœ‰ mask ç‚¹çš„æ¬§æ°è·ç¦»: (H, W, N, 1)
    dists = torch.sqrt((y_coords - mask_y)**2 + (x_coords - mask_x)**2)  # (H, W, N, 1)
    dists = dists.squeeze(-1)  # (H, W, N)

    # è®¡ç®—æƒé‡: w = 1 - (d / max_dist), è·ç¦»è¶…è¿‡ max_dist çš„è®¾ä¸º 0
    weights = 1 - dists / max_dist
    weights = torch.clamp(weights, min=0.0)  # (H, W, N)

    # åŠ æƒå‡å€¼ï¼ˆä¸åŒ…å«å½“å‰åƒç´ çš„æƒ…å†µï¼‰
    # æ³¨æ„ï¼šmask_logits_vals æ˜¯ (N,)ï¼Œéœ€è¦æ‰©å±•ä¸º (1, 1, N)
    mask_logits_vals_exp = mask_logits_vals.unsqueeze(0).unsqueeze(0)  # (1, 1, N)

    # åŠ æƒå’Œä¸æƒé‡å’Œï¼ˆç”¨äºå‡å€¼ï¼‰
    weighted_sum = torch.sum(weights * mask_logits_vals_exp, dim=-1)      # (H, W)
    total_weight = torch.sum(weights, dim=-1)                             # (H, W)

    # é˜²æ­¢é™¤é›¶
    safe_total_weight = torch.where(total_weight > 0, total_weight, torch.ones_like(total_weight))
    mean_wo = weighted_sum / safe_total_weight  # (H, W)

    # è®¡ç®—æ–¹å·®ï¼šVar = sum(w * (x - mean)^2) / sum(w)
    # å…ˆè®¡ç®— (x - mean)^2ï¼Œæ³¨æ„ mean æ˜¯ (H, W)ï¼Œéœ€è¦æ‰©å±•ä¸º (H, W, N)
    mean_exp = mean_wo.unsqueeze(-1)  # (H, W, 1)
    squared_diff = (mask_logits_vals_exp - mean_exp) ** 2  # (H, W, N)

    # åŠ æƒæ–¹å·®ï¼ˆä¸åŒ…å«å½“å‰åƒç´ ï¼‰
    var_numerator_wo = torch.sum(weights * squared_diff, dim=-1)
    var_wo = torch.where(total_weight > 0, var_numerator_wo / safe_total_weight, torch.zeros_like(var_numerator_wo))

    # =====================================================
    # ç¬¬äºŒç§æƒ…å†µï¼šå°†å½“å‰åƒç´ ä¹ŸåŠ å…¥æ ·æœ¬ä¸­
    # =====================================================

    # å½“å‰åƒç´ çš„ logits å€¼: (H, W)
    current_logits = logits  # (H, W)

    # å°†å½“å‰åƒç´ è§†ä¸ºé¢å¤–æ ·æœ¬ï¼Œæ·»åŠ åˆ° mask æ ·æœ¬é›†ä¸­
    # æ„é€ æ–°çš„ logits å€¼: (H, W, N+1)
    current_logits_exp = current_logits.unsqueeze(-1)  # (H, W, 1)
    extended_logits = torch.cat([mask_logits_vals_exp.expand(H, W, N), current_logits_exp], dim=-1)  # (H, W, N+1)

    # æ„é€ æ–°çš„æƒé‡: åŸ weights (H, W, N)ï¼ŒåŠ ä¸Šå½“å‰åƒç´ åˆ°è‡ªèº«çš„è·ç¦»æƒé‡
    # å½“å‰åƒç´ åˆ° mask ä¸­æ¯ä¸ªç‚¹çš„è·ç¦»å·²ç»è®¡ç®—è¿‡ï¼Œç°åœ¨è¦è®¡ç®—å½“å‰åƒç´ åˆ°è‡ªèº«çš„æƒé‡ï¼Ÿ
    # æ³¨æ„ï¼šå½“å‰åƒç´ åˆ°è‡ªå·±çš„è·ç¦»ä¸º 0ï¼Œæƒé‡ä¸º 1 - 0/max_dist = 1
    # ä½†æˆ‘ä»¬è¿˜è¦è®¡ç®—å½“å‰åƒç´ åˆ°æ¯ä¸ª mask ç‚¹çš„è·ç¦»ï¼Ÿä¸ï¼Œæˆ‘ä»¬åªéœ€è¦å®ƒè‡ªå·±çš„æƒé‡é¡¹
    # å®é™…ä¸Šï¼Œæˆ‘ä»¬åªéœ€æ·»åŠ ä¸€ä¸ªæ–°çš„æƒé‡ç»´åº¦ï¼šæƒé‡ä¸º 1.0ï¼ˆå› ä¸º d=0ï¼‰

    # æ–°çš„æƒé‡å¼ é‡: (H, W, N+1)
    current_weight = torch.ones((H, W, 1), device=logits.device)  # (H, W, 1)
    extended_weights = torch.cat([weights, current_weight], dim=-1)  # (H, W, N+1)

    # è®¡ç®—æ–°å‡å€¼
    extended_weighted_sum = torch.sum(extended_weights * extended_logits, dim=-1)
    extended_total_weight = torch.sum(extended_weights, dim=-1)
    safe_extended_weight = torch.where(extended_total_weight > 0, extended_total_weight, torch.ones_like(extended_total_weight))
    mean_w = extended_weighted_sum / safe_extended_weight  # (H, W)

    # è®¡ç®—æ–°æ–¹å·®
    mean_w_exp = mean_w.unsqueeze(-1)  # (H, W, 1)
    squared_diff_w = (extended_logits - mean_w_exp) ** 2  # (H, W, N+1)
    var_numerator_w = torch.sum(extended_weights * squared_diff_w, dim=-1)
    var_w = torch.where(extended_total_weight > 0, var_numerator_w / safe_extended_weight, torch.zeros_like(var_numerator_w))

    return var_wo, var_w


def compute_weighted_variance_v1(
    logits: torch.Tensor,
    mask: torch.Tensor,
    channel_weights: torch.Tensor,
    top_k: int = None,
    thre: float = 0.0
):
    """
    è®¡ç®—æ¯ä¸ªåƒç´ ä½ç½®åœ¨ multi-channel mask åŒºåŸŸå†…åŸºäºè·ç¦»å’Œé€šé“æƒé‡è”åˆåŠ æƒçš„æ–¹å·®ã€‚

    Args:
        logits: (H, W) æˆ– (C, H, W) çš„ Tensorã€‚è‹¥ä¸º (C,H,W)ï¼Œåˆ™æŒ‰é€šé“å–å€¼ï¼›è‹¥ä¸º (H,W)ï¼Œåˆ™å…±äº«
        mask: (C, H, W) çš„äºŒå€¼ Tensor
        channel_weights: (C,) çš„ Tensorï¼Œè¡¨ç¤ºæ¯ä¸ªé€šé“çš„æƒé‡
        top_k: int, ä½¿ç”¨æœ€è¿‘çš„ top_k ä¸ªç‚¹ï¼ˆè·¨é€šé“ï¼‰
        thre: float, å½“æ‰€æœ‰é€šé“æ€»å…±åªæœ‰ä¸€ä¸ªæœ‰æ•ˆç‚¹æ—¶ï¼Œç”¨äº fallback çš„è™šæ‹Ÿç‚¹å–å€¼

    Returns:
        var_wo: (H, W) ä¸åŒ…å«å½“å‰åƒç´ çš„åŠ æƒæ–¹å·®
        var_w:  (H, W) åŒ…å«å½“å‰åƒç´ çš„åŠ æƒæ–¹å·®
    """
    assert mask.ndim == 3, "mask must be (C, H, W)"
    C, H, W = mask.shape

    if logits.shape == (H, W):
        # æ‰©å±• logits åˆ° (C, H, W)
        logits_exp = logits.unsqueeze(0).expand(C, H, W)
    elif logits.shape == (C, H, W):
        logits_exp = logits
    else:
        raise ValueError(f"logits must be (H, W) or (C, H, W), got {logits.shape}")

    assert channel_weights.shape == (C,), f"channel_weights must be (C,), got {channel_weights.shape}"

    # é¢„è®¡ç®—åæ ‡
    y_coords, x_coords = torch.meshgrid(torch.arange(H, device=mask.device),
                                        torch.arange(W, device=mask.device),
                                        indexing='ij')
    y_coords = y_coords.unsqueeze(0).unsqueeze(0)  # (1, 1, H, W)
    x_coords = x_coords.unsqueeze(0).unsqueeze(0)

    # è·å–æ‰€æœ‰ mask ä¸º True çš„ç‚¹ï¼š(N, 3) -> [c, y, x]
    indices = mask.nonzero(as_tuple=False)  # (N, 3)
    N = indices.shape[0]

    if N == 0:
        var_wo = torch.zeros(H, W, device=mask.device)
        var_w = torch.zeros(H, W, device=mask.device)
        return var_wo, var_w

    # æå–æ¯ä¸ªæœ‰æ•ˆç‚¹çš„ c, y, x
    c_idx = indices[:, 0]  # (N,)
    y_idx = indices[:, 1]  # (N,)
    x_idx = indices[:, 2]  # (N,)

    # è·å–è¿™äº›ç‚¹çš„ logits å€¼
    point_logits = logits_exp[c_idx, y_idx, x_idx]  # (N,)

    # è·å–æ¯ä¸ªç‚¹çš„é€šé“æƒé‡
    point_channel_weights = channel_weights[c_idx]  # (N,)

    # è®¡ç®—æ¯ä¸ªå½“å‰åƒç´  (i,j) åˆ°æ¯ä¸ª mask ç‚¹çš„è·ç¦»
    # å½“å‰åæ ‡: (1, 1, H, W)
    # ç‚¹åæ ‡: (N,) -> (N, 1, 1)
    y_idx = y_idx.view(N, 1, 1)
    x_idx = x_idx.view(N, 1, 1)
    dists = torch.sqrt((y_coords - y_idx)**2 + (x_coords - x_idx)**2)  # (N, H, W)
    dists = dists.squeeze(0)
    dists = dists.permute(1, 2, 0)  # (H, W, N)

    # === ç‰¹æ®Šæƒ…å†µï¼šä»…æœ‰ä¸€ä¸ªæœ‰æ•ˆç‚¹ ===
    if N == 1:
        # æ·»åŠ ä¸€ä¸ªè™šæ‹Ÿç‚¹ï¼šè·ç¦»ç›¸åŒï¼Œlogits = threï¼Œchannel_weight = è¯¥ç‚¹çš„æƒé‡
        dists_used = torch.cat([dists, dists], dim=-1)  # (H, W, 2)

        # å½’ä¸€åŒ–
        d_min = dists_used.min(dim=-1, keepdim=True).values
        d_max = dists_used.max(dim=-1, keepdim=True).values
        eps = 1e-8
        normalized_d = (dists_used - d_min) / (d_max - d_min + eps)
        dist_weights = 1 - normalized_d  # (H, W, 2)
        dist_weights = torch.clamp(dist_weights, min=0.0)

        # ç»¼åˆæƒé‡ = è·ç¦»æƒé‡ Ã— é€šé“æƒé‡ï¼ˆå¹¿æ’­ï¼‰
        # åŸå§‹ç‚¹æƒé‡
        orig_cw = point_channel_weights[0]  # scalar
        cw_weights = torch.tensor([orig_cw, orig_cw], device=mask.device)  # (2,)
        cw_weights = cw_weights.view(1, 1, 2).expand(H, W, 2)  # (H, W, 2)
        weights = dist_weights * cw_weights  # (H, W, 2)

        # logits å€¼
        val0 = point_logits[0]
        logits_vals = torch.full((H, W, 2), thre, device=mask.device)
        logits_vals[:, :, 0] = val0  # (H, W, 2)

    else:
        # æ­£å¸¸æƒ…å†µï¼šN >= 2
        k = min(top_k, N) if top_k is not None else N

        # å½’ä¸€åŒ–è·ç¦»ï¼ˆæ¯ä¸ªåƒç´ ç‹¬ç«‹ï¼‰
        d_min = dists.min(dim=-1, keepdim=True).values    # (H, W, 1)
        d_max = dists.max(dim=-1, keepdim=True).values
        eps = 1e-8
        normalized_d = (dists - d_min) / (d_max - d_min + eps)
        dist_weights = 1 - normalized_d  # (H, W, N)
        dist_weights = torch.clamp(dist_weights, min=0.0)

        # ç»¼åˆæƒé‡ï¼šè·ç¦»æƒé‡ Ã— é€šé“æƒé‡
        # point_channel_weights: (N,) -> (1, 1, N)
        cw_exp = point_channel_weights.view(1, 1, -1).expand(H, W, N)
        weights = dist_weights * cw_exp  # (H, W, N)

        # å¯é€‰ï¼šåªå– top_k
        if k < N:
            topk_weights, topk_indices = torch.topk(weights, k, dim=-1)  # (H, W, k)
            logits_vals = point_logits.unsqueeze(0).unsqueeze(0).expand(H, W, N)
            logits_vals = torch.gather(logits_vals, dim=-1, index=topk_indices)
            weights = topk_weights
            dists_used = torch.gather(dists, dim=-1, index=topk_indices)
        else:
            logits_vals = point_logits.unsqueeze(0).unsqueeze(0).expand(H, W, N)
            dists_used = dists

    # === æƒ…å†µä¸€ï¼šä¸åŒ…å«å½“å‰åƒç´ çš„åŠ æƒæ–¹å·® ===
    weighted_sum = torch.sum(weights * logits_vals, dim=-1)  # (H, W)
    total_weight = torch.sum(weights, dim=-1)                # (H, W)
    safe_weight = torch.where(total_weight > 0, total_weight, torch.ones_like(total_weight))
    mean_wo = weighted_sum / safe_weight  # (H, W)

    mean_exp = mean_wo.unsqueeze(-1)  # (H, W, 1)
    squared_diff = (logits_vals - mean_exp) ** 2
    var_numerator_wo = torch.sum(weights * squared_diff, dim=-1)
    var_wo = torch.where(total_weight > 0, var_numerator_wo / safe_weight, torch.zeros_like(var_numerator_wo))

    # === æƒ…å†µäºŒï¼šåŒ…å«å½“å‰åƒç´  ===
    current_logits = logits if logits.ndim == 2 else logits.mean(dim=0)  # (H, W)
    current_logits = current_logits.unsqueeze(-1)  # (H, W, 1)

    # è®¡ç®—å½“å‰åƒç´ åˆ°å„ mask ç‚¹çš„è·ç¦»ï¼ˆç”¨äºæƒé‡ï¼‰
    if N == 1:
        # ä½¿ç”¨ä¸åŸç‚¹ç›¸åŒçš„ distsï¼ˆå·²æ‰©å±•ï¼‰
        current_dists = dists  # (H, W, 1)
        d_min_cur = current_dists
        d_max_cur = current_dists
        normalized_cur_d = (torch.zeros_like(d_min_cur) - d_min_cur) / (d_max_cur - d_min_cur + eps)
        dist_weight_cur = 1 - normalized_cur_d
        # é€šé“æƒé‡ï¼šä¸åŸç‚¹ç›¸åŒ
        cw_cur = point_channel_weights[0].view(1, 1, 1).expand(H, W, 1)
        current_weight_val = dist_weight_cur * cw_cur
    else:
        # ä½¿ç”¨ dists_used (H, W, k or N)
        d_min_cur = dists_used.min(dim=-1, keepdim=True).values
        d_max_cur = dists_used.max(dim=-1, keepdim=True).values
        current_dists = torch.zeros_like(d_min_cur)
        normalized_cur_d = (current_dists - d_min_cur) / (d_max_cur - d_min_cur + eps)
        dist_weight_cur = 1 - normalized_cur_d
        dist_weight_cur = torch.clamp(dist_weight_cur, min=0.0)
        # é€šé“æƒé‡ï¼šæš‚ç”¨å¹³å‡æˆ–æœ€å¤§ï¼Ÿè¿™é‡Œæˆ‘ä»¬ä¸åŒºåˆ†ï¼Œåªç”¨è·ç¦» + ç»Ÿä¸€é€šé“æƒé‡é€»è¾‘
        # å®é™…ä¸Šï¼Œå½“å‰åƒç´ ä¸å±äºä»»ä½•é€šé“ï¼Œæˆ‘ä»¬åªè€ƒè™‘å…¶è·ç¦»æƒé‡ Ã— ï¼Ÿï¼Ÿ
        # æ›´åˆç†ï¼šå½“å‰åƒç´ ä¸å¸¦é€šé“æƒé‡ï¼Œåªä¿ç•™è·ç¦»æƒé‡éƒ¨åˆ†
        # æ‰€ä»¥æˆ‘ä»¬åªç”¨ dist_weight_curï¼Œä¸å†ä¹˜ channel weight
        current_weight_val = dist_weight_cur  # (H, W, 1)

    # æ‹¼æ¥
    extended_logits = torch.cat([logits_vals, current_logits], dim=-1)  # (H, W, *)
    extended_weights = torch.cat([weights, current_weight_val], dim=-1)  # (H, W, *)

    # æ–°å‡å€¼
    ext_sum = torch.sum(extended_weights * extended_logits, dim=-1)
    ext_weight_sum = torch.sum(extended_weights, dim=-1)
    safe_ext_weight = torch.where(ext_weight_sum > 0, ext_weight_sum, torch.ones_like(ext_weight_sum))
    mean_w = ext_sum / safe_ext_weight

    # æ–°æ–¹å·®
    mean_w_exp = mean_w.unsqueeze(-1)
    squared_diff_w = (extended_logits - mean_w_exp) ** 2
    var_numerator_w = torch.sum(extended_weights * squared_diff_w, dim=-1)
    var_w = torch.where(ext_weight_sum > 0, var_numerator_w / safe_ext_weight, torch.zeros_like(var_numerator_w))

    return var_wo, var_w

def compute_weighted_variance_v2(
    logits: torch.Tensor,
    mask: torch.Tensor,
    top_k: int = None,
    thre: float = 0.0
):
    """
    è®¡ç®—æ¯ä¸ªåƒç´ ä½ç½®åœ¨ mask åŒºåŸŸå†…åŸºäºè·ç¦»åŠ æƒçš„æ–¹å·®ï¼ˆä¸¤ç§æƒ…å†µï¼‰ï¼š
    1. ä¸åŒ…å«å½“å‰åƒç´ çš„åŠ æƒæ–¹å·®
    2. åŒ…å«å½“å‰åƒç´ çš„åŠ æƒæ–¹å·®
    ä½¿ç”¨ mask ä¸­è·ç¦»æœ€è¿‘çš„ top_k ä¸ªç‚¹å‚ä¸è®¡ç®—ã€‚
    è·ç¦»æƒé‡å½’ä¸€åŒ–åŸºäºï¼šå½“å‰åƒç´ åˆ°è¿™äº› mask ç‚¹çš„ min å’Œ max è·ç¦»ã€‚

    ç‰¹æ®Šå¤„ç†ï¼šå½“ mask ä¸­åªæœ‰ä¸€ä¸ªç‚¹æ—¶ï¼Œæ·»åŠ ä¸€ä¸ªè™šæ‹Ÿç‚¹ï¼ˆå€¼=threï¼Œæƒé‡åŒåŸç‚¹ï¼‰ç”¨äºè®¡ç®—æ–¹å·®ã€‚

    Args:
        logits: (H, W) çš„ Tensor
        mask: (H, W) çš„äºŒå€¼ Tensor (0 or 1)
        top_k: int, ä½¿ç”¨ mask ä¸­è·ç¦»æœ€è¿‘çš„ top_k ä¸ªç‚¹ï¼›è‹¥ä¸º Noneï¼Œåˆ™ä½¿ç”¨æ‰€æœ‰ mask ç‚¹
        thre: float, å½“ mask åªæœ‰ä¸€ä¸ªç‚¹æ—¶ï¼Œæ·»åŠ çš„è™šæ‹Ÿç‚¹çš„å–å€¼ï¼ˆç”¨äº fallbackï¼‰

    Returns:
        var_wo: (H, W) ä¸åŒ…å«å½“å‰åƒç´ çš„åŠ æƒæ–¹å·®
        var_w:  (H, W) åŒ…å«å½“å‰åƒç´ çš„åŠ æƒæ–¹å·®
    """
    assert logits.shape == mask.shape, "logits and mask must have the same shape"
    H, W = logits.shape

    # é¢„è®¡ç®—æ‰€æœ‰åƒç´ åæ ‡
    y_coords, x_coords = torch.meshgrid(torch.arange(H, device=logits.device),
                                        torch.arange(W, device=logits.device),
                                        indexing='ij')  # (H, W)
    y_coords = y_coords.unsqueeze(-1).unsqueeze(-1)  # æ‰©å±•åæ ‡ä¸º (H, W, 1, 1) ä¾¿äºå¹¿æ’­
    x_coords = x_coords.unsqueeze(-1).unsqueeze(-1)

    # è·å– mask ä¸­ä¸º 1 çš„æ‰€æœ‰ä½ç½®çš„åæ ‡
    mask_positions = mask.nonzero(as_tuple=False)  # (N, 2)
    N = mask_positions.shape[0]

    # æ­£å¸¸æƒ…å†µï¼šN >= 4
    k = min(top_k, N) if top_k is not None else N
    use_all = k >= N

    # æå– mask åŒºåŸŸå†…çš„ logits å€¼
    mask_logits_vals = logits[mask_positions[:, 0], mask_positions[:, 1]]  # (N,)

    # æ„é€  mask åæ ‡å¼ é‡
    mask_y = mask_positions[:, 0].view(1, 1, -1, 1)  # (1, 1, N, 1)
    mask_x = mask_positions[:, 1].view(1, 1, -1, 1)

    # è®¡ç®—è·ç¦»: (H, W, N)
    dists = torch.sqrt((y_coords - mask_y)**2 + (x_coords - mask_x)**2).squeeze(-1)  # (H, W, N)

    if not use_all:
        # å–æœ€è¿‘çš„ k ä¸ªç‚¹
        topk_dists, topk_indices = torch.topk(dists, k, dim=-1, largest=False)  # (H, W, k)
        mask_logits_vals_exp = mask_logits_vals.unsqueeze(0).unsqueeze(0).expand(H, W, N)
        topk_logits = torch.gather(mask_logits_vals_exp, dim=-1, index=topk_indices)  # (H, W, k)
        dists_used = topk_dists
    else:
        topk_logits = mask_logits_vals.unsqueeze(0).unsqueeze(0).expand(H, W, N)  # (H, W, N)
        dists_used = dists  # (H, W, k)

        # åŠ¨æ€å½’ä¸€åŒ–
        d_min = dists_used.min(dim=-1, keepdim=True).values
        d_max = dists_used.max(dim=-1, keepdim=True).values
        eps = 1e-8
        normalized_d = (dists_used - d_min) / (d_max - d_min + eps)
        weights = 1 - normalized_d
        weights = torch.clamp(weights, min=0.0)  # (H, W, k)

    # === æƒ…å†µä¸€ï¼šä¸åŒ…å«å½“å‰åƒç´ çš„åŠ æƒæ–¹å·® ===
    weighted_sum = torch.sum(weights * topk_logits, dim=-1)      # (H, W)
    total_weight = torch.sum(weights, dim=-1)                    # (H, W)
    safe_total_weight = torch.where(total_weight > 0, total_weight, torch.ones_like(total_weight))
    mean_wo = weighted_sum / safe_total_weight  # (H, W)

    mean_exp = mean_wo.unsqueeze(-1)  # (H, W, 1)
    squared_diff = (topk_logits - mean_exp) ** 2  # (H, W, *)
    var_numerator_wo = torch.sum(weights * squared_diff, dim=-1)
    var_wo = torch.where(total_weight > 0, var_numerator_wo / safe_total_weight, torch.zeros_like(var_numerator_wo))

    # === æƒ…å†µäºŒï¼šåŒ…å«å½“å‰åƒç´  ===
    current_logits = logits.unsqueeze(-1)  # (H, W, 1)

    # è®¡ç®—å½“å‰åƒç´ åˆ° mask åŒºåŸŸçš„è·ç¦»ï¼ˆç”¨äº fallback æƒé‡ï¼‰
    if N == 1:
        # å½“å‰åƒç´ åˆ°å”¯ä¸€ mask ç‚¹çš„è·ç¦»
        y0, x0 = mask_positions[0]
        current_dists = torch.sqrt((y_coords.squeeze() - y0)**2 + (x_coords.squeeze() - x0)**2)  # (H, W)
        current_dists = current_dists.unsqueeze(-1)  # (H, W, 1)
        # å½’ä¸€åŒ–æ—¶ä½¿ç”¨ä¸åŸç‚¹ç›¸åŒçš„ d_min/d_maxï¼ˆå³è‡ªèº«è·ç¦»ï¼‰
        d_min_cur = current_dists
        d_max_cur = current_dists
        normalized_cur_d = (current_dists - d_min_cur) / (d_max_cur - d_min_cur + eps)  # 0
        current_weight_val = 1 - normalized_cur_d  # (H, W, 1)
    else:
        # æ­£å¸¸æƒ…å†µï¼šä½¿ç”¨å½“å‰åƒç´ åˆ°æ‰€é€‰ mask ç‚¹çš„è·ç¦»
        current_dists = dists_used  # å·²ç»æ˜¯ (H, W, k) æˆ– (H, W, 2)
        d_min_cur = d_min
        d_max_cur = d_max
        eps = 1e-8
        normalized_cur_d = (torch.zeros_like(d_min_cur) - d_min_cur) / (d_max_cur - d_min_cur + eps)
        current_weight_val = 1 - normalized_cur_d
        current_weight_val = torch.clamp(current_weight_val, min=0.0)

    # æ‹¼æ¥å½“å‰åƒç´ 
    extended_logits = torch.cat([topk_logits, current_logits], dim=-1)  # (H, W, k+1 æˆ– 3)
    extended_weights = torch.cat([weights, current_weight_val], dim=-1)  # (H, W, k+1 æˆ– 3)

    # æ–°å‡å€¼
    extended_weighted_sum = torch.sum(extended_weights * extended_logits, dim=-1)
    extended_total_weight = torch.sum(extended_weights, dim=-1)
    safe_extended_weight = torch.where(extended_total_weight > 0, extended_total_weight, torch.ones_like(extended_total_weight))
    mean_w = extended_weighted_sum / safe_extended_weight  # (H, W)
    # print(mean_w)

    # æ–°æ–¹å·®
    mean_w_exp = mean_w.unsqueeze(-1)
    squared_diff_w = (extended_logits - mean_w_exp) ** 2
    var_numerator_w = torch.sum(extended_weights * squared_diff_w, dim=-1)
    var_w = torch.where(extended_total_weight > 0, var_numerator_w / safe_extended_weight, torch.zeros_like(var_numerator_w))

    return var_wo, var_w

def random_select_from_mask(mask, num):
    """
    ä» mask ä¸­å€¼ä¸º 1 çš„ä½ç½®éšæœºé€‰æ‹© num ä¸ªï¼Œè¿”å›æ–°çš„ maskã€‚
    
    å‚æ•°:
        mask (torch.Tensor): å€¼ä¸º 0 æˆ– 1 çš„äºŒå€¼ maskï¼Œå½¢çŠ¶ä¸º [H, W] æˆ– [1, H, W] ç­‰
        num (int): è¦é€‰æ‹©çš„åƒç´ æ•°é‡
    
    è¿”å›:
        torch.Tensor: æ–°çš„ maskï¼Œåªä¿ç•™éšæœºé€‰ä¸­çš„ num ä¸ªåƒç´ ï¼ˆå€¼ä¸º 1ï¼‰
    """
    # ç¡®ä¿ mask æ˜¯ bool æˆ– 0/1 çš„æ•´æ•°ç±»å‹
    if mask.dtype != torch.bool:
        mask_bool = mask.bool()
    else:
        mask_bool = mask

    # è·å–å€¼ä¸º 1 çš„åƒç´ çš„ç´¢å¼•
    indices = torch.nonzero(mask_bool, as_tuple=False)  # å½¢çŠ¶ä¸º [N, dim]

    # å¦‚æœ 1 çš„æ•°é‡å°‘äºæˆ–ç­‰äº numï¼Œç›´æ¥è¿”å›åŸ maskï¼ˆæˆ–å…¨é€‰ï¼‰
    if indices.size(0) <= num:
        return mask.clone()

    # éšæœºæ‰“ä¹±å¹¶é€‰æ‹© num ä¸ªç´¢å¼•
    perm = torch.randperm(indices.size(0), device=mask.device)
    selected_indices = indices[perm[:num]]

    # åˆ›å»ºè¾“å‡º maskï¼Œåˆå§‹åŒ–ä¸º 0
    out_mask = torch.zeros_like(mask, dtype=torch.bool)

    # å°†é€‰ä¸­çš„ä½ç½®è®¾ä¸º True
    if mask.dim() == 2:
        out_mask[selected_indices[:, 0], selected_indices[:, 1]] = True
    elif mask.dim() == 3:
        out_mask[selected_indices[:, 0], selected_indices[:, 1], selected_indices[:, 2]] = True
    else:
        raise ValueError("åªæ”¯æŒ 2D æˆ– 3D çš„ mask")

    return out_mask


def random_select_from_prob_mask(prob_mask, num, replacement=False):
    """
    æ ¹æ® float ç±»å‹çš„æ¦‚ç‡ maskï¼ŒæŒ‰æ¦‚ç‡æƒé‡éšæœºé€‰æ‹© num ä¸ªåƒç´ ï¼Œè¿”å›äºŒå€¼ maskã€‚

    å‚æ•°:
        prob_mask (torch.Tensor): float ç±»å‹ï¼Œå½¢çŠ¶ä¸º [H, W] æˆ– [C, H, W]ï¼ˆC=1ï¼‰ï¼Œå€¼åœ¨ [0,1] è¡¨ç¤ºé€‰ä¸­æ¦‚ç‡
        num (int): è¦é€‰æ‹©çš„åƒç´ æ•°é‡
        replacement (bool): æ˜¯å¦å…è®¸é‡å¤é‡‡æ ·ï¼ˆé€šå¸¸ Falseï¼‰

    è¿”å›:
        torch.Tensor: äºŒå€¼ maskï¼Œlong ç±»å‹ï¼Œé€‰ä¸­çš„ä½ç½®ä¸º 1ï¼Œå…¶ä½™ä¸º 0
    """
    # ä¿å­˜åŸå§‹è®¾å¤‡å’Œå½¢çŠ¶
    device = prob_mask.device
    shape = prob_mask.shape

    # å±•å¹³ä¸ºä¸€ç»´ä¾¿äºé‡‡æ ·
    if prob_mask.dim() == 3:
        if prob_mask.size(0) != 1:
            raise ValueError("3D mask å¿…é¡»æ˜¯ [1, H, W] å½¢å¼")
        flat_probs = prob_mask.view(-1)  # [C*H*W]
    elif prob_mask.dim() == 2:
        flat_probs = prob_mask.reshape(-1)  # [H*W]
    else:
        raise ValueError("åªæ”¯æŒ 2D æˆ– 3D çš„ prob_mask")

    # æ£€æŸ¥ num æ˜¯å¦åˆæ³•
    total_elements = flat_probs.numel()
    if num > total_elements and not replacement:
        raise ValueError(f"æ— æ³•åœ¨ä¸æ”¾å›çš„æƒ…å†µä¸‹é€‰æ‹© {num} ä¸ªå…ƒç´ ï¼ˆæ€»å…±åªæœ‰ {total_elements} ä¸ªï¼‰")

    # ä½¿ç”¨ multinomial æŒ‰æ¦‚ç‡é‡‡æ ·ï¼ˆæ”¯æŒé›¶æ¦‚ç‡ï¼‰
    try:
        indices = torch.multinomial(flat_probs, num, replacement=replacement)
    except RuntimeError as e:
        if "multinomial" in str(e) and not replacement:
            # å¯èƒ½æ˜¯å› ä¸ºéé›¶æ•°å°‘äº num ä¸”ä¸æ”¾å›
            # æˆ‘ä»¬å¯ä»¥å…ˆå½’ä¸€åŒ–éé›¶éƒ¨åˆ†ï¼Œæˆ–æŠ¥æ›´å‹å¥½é”™è¯¯
            nonzero_count = (flat_probs > 0).sum().item()
            if nonzero_count < num:
                raise ValueError(
                    f"éé›¶æ¦‚ç‡çš„ä½ç½®åªæœ‰ {nonzero_count} ä¸ªï¼Œ"
                    f"ä½†è¦æ±‚é€‰æ‹© {num} ä¸ªåƒç´ ï¼Œè¯·ç¡®ä¿ num ä¸è¶…è¿‡éé›¶ä½ç½®æ•°ï¼Œ"
                    f"æˆ–ä½¿ç”¨ replacement=True"
                ) from e
            else:
                raise e
        else:
            raise e

    # åˆ›å»ºè¾“å‡º mask
    out_mask = torch.zeros_like(flat_probs)
    out_mask[indices] = 1

    # æ¢å¤åŸå§‹å½¢çŠ¶
    out_mask = out_mask.reshape(shape)

    return out_mask

# è¾…åŠ©å‡½æ•°ï¼šç”¨äº fallback çš„ç‹¬ç«‹å‡åŒ€é‡‡æ ·å‡½æ•°
def select_uniform_logits_pixels_v2(logits, mask_a, mask_b, num, num_bins=20):
    """ç®€åŒ–ç‰ˆï¼šåœ¨ mask_a \ mask_b ä¸­æŒ‰å€¼åŸŸåˆ†æ¡¶å‡åŒ€é‡‡æ ·"""
    if logits.dim() == 3:
        logits = logits.squeeze(0)
    H, W = logits.shape

    if mask_a.dtype == torch.int:
        mask_a = mask_a.bool()
    if mask_b.dtype == torch.int:
        mask_b = mask_b.bool()

    candidate_mask = mask_a & (~mask_b)
    candidate_indices = candidate_mask.nonzero(as_tuple=False)
    N = candidate_indices.size(0)
    if N == 0 or num == 0:
        return mask_b.clone()
    if num > N:
        num = N

    candidate_logits = logits[candidate_mask]
    min_val = candidate_logits.min().item()
    max_val = candidate_logits.max().item()

    if abs(max_val - min_val) < 1e-6:
        idx = torch.randperm(N)[:num]
        selected = candidate_indices[idx]
    else:
        bin_edges = torch.linspace(min_val, max_val, num + 1, device=logits.device)
        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
        selected = []
        for center in bin_centers:
            dist = torch.abs(candidate_logits - center)
            best = torch.argmin(dist)
            selected.append(candidate_indices[best])
            # ç§»é™¤å·²é€‰
            if len(selected) < num:
                keep = torch.ones(candidate_logits.shape, dtype=torch.bool, device=logits.device)
                keep[best] = False
                candidate_logits = candidate_logits[keep]
                candidate_indices = candidate_indices[keep]
        selected = torch.stack(selected)
    mask_c = mask_b.clone()
    for h, w in selected:
        mask_c[h, w] = True
    return mask_c

def select_complementary_pixels(logits, mask_a, mask_b, num, num_bins=20):
    """
    é€‰æ‹© num ä¸ªæ–°åƒç´ ï¼Œä½¿å¾—å®ƒä»¬ä¸ mask_b ä¸­çš„åƒç´ åˆå¹¶åï¼Œ
    å¯¹åº”çš„ logits å€¼åœ¨æ•°å€¼ä¸Šå°½å¯èƒ½å‡åŒ€åˆ†å¸ƒã€‚

    å‚æ•°ï¼š
        logits: [H, W] æˆ– [1, H, W]
        mask_a: bool [H, W], å€™é€‰åŒºåŸŸ
        mask_b: bool [H, W], mask_b âŠ† mask_a
        num: int, è¦æ–°å¢çš„åƒç´ æ•°
        num_bins: int, ç”¨äºç»Ÿè®¡åˆ†å¸ƒçš„æ¡¶æ•°

    è¿”å›ï¼š
        mask_c: bool [H, W], mask_b + æ–°é€‰çš„ num ä¸ªåƒç´ 
    """
    if mask_a.dtype == torch.int:
        mask_a = mask_a.bool()
    if mask_b.dtype == torch.int:
        mask_b = mask_b.bool()

    if logits.dim() == 3:
        logits = logits.squeeze(0)
    H, W = logits.shape

    # 1. è·å– mask_b ä¸­çš„ logits å€¼ï¼ˆå·²æœ‰æ ·æœ¬ï¼‰
    existing_mask = mask_b
    existing_logits = logits[existing_mask]  # [M]
    M = existing_logits.numel()

    # å¦‚æœæ²¡æœ‰ç°æœ‰æ ·æœ¬ï¼Œé€€åŒ–ä¸ºåœ¨å€™é€‰ä¸­å‡åŒ€é‡‡æ ·
    if M == 0:
        return select_uniform_logits_pixels_v2(logits, mask_a, mask_b, num, num_bins)

    # 2. å®šä¹‰å…¨å±€å€¼åŸŸï¼šåŸºäº mask_a ä¸­çš„æ‰€æœ‰å€¼
    universe_mask = mask_a
    global_logits = logits[universe_mask]
    
    # ğŸ‘‰ æ£€æŸ¥ global_logits æ˜¯å¦ä¸ºç©º
    if global_logits.numel() == 0:
        # mask_a ä¸­æ²¡æœ‰æœ‰æ•ˆåƒç´ ï¼Œæ— æ³•é€‰æ‹©ï¼Œç›´æ¥è¿”å› mask_b
        print("Warning: mask_a has no active pixels. Returning mask_b.")
        return mask_b.clone()

    min_val = global_logits.min().item()
    max_val = global_logits.max().item()

    if abs(max_val - min_val) < 1e-6:
        # æ‰€æœ‰å€¼å‡ ä¹ç›¸ç­‰ï¼Œéšä¾¿é€‰
        candidate_mask = mask_a & (~mask_b)
        candidate_indices = candidate_mask.nonzero()
        N = candidate_indices.size(0)
        if N == 0:
            return mask_b.clone()
        num = min(num, N)
        selected_idx = torch.randperm(N)[:num]
        selected_coords = candidate_indices[selected_idx]
    else:
        # 3. åˆ’åˆ† bins
        bin_edges = torch.linspace(min_val, max_val, steps=num_bins + 1, device=logits.device)
        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2  # [num_bins]

        # 4. ç»Ÿè®¡ mask_b åœ¨å„ bin ä¸­çš„æ•°é‡
        bin_indices_existing = torch.bucketize(existing_logits, bin_edges, right=False) - 1
        bin_indices_existing = torch.clamp(bin_indices_existing, 0, num_bins - 1)
        bin_counts = torch.bincount(bin_indices_existing, minlength=num_bins).float()  # [num_bins]

        # 5. æŒ‰â€œè°æœ€ç¼ºâ€æ’åºï¼šä¼˜å…ˆè¡¥å……æ ·æœ¬å°‘çš„æ¡¶
        # æˆ‘ä»¬å¸Œæœ›æ¯ä¸ªæ¡¶æœ€ç»ˆæœ‰ roughly (M + num) / num_bins ä¸ªæ ·æœ¬
        desired_per_bin = (M + num) / num_bins
        deficit = desired_per_bin - bin_counts  # ç¼ºå¤šå°‘
        _, sorted_deficit_bins = torch.sort(deficit, descending=True)  # ä»ç¼ºå¾—æœ€å¤šåˆ°å°‘

        # 6. å€™é€‰åŒºåŸŸï¼šmask_a ä¸”ä¸åœ¨ mask_b
        candidate_mask = mask_a & (~mask_b)
        candidate_indices = candidate_mask.nonzero(as_tuple=False)  # [N, 2]
        N = candidate_indices.size(0)

        if N == 0:
            print("No candidates available.")
            return mask_b.clone()

        candidate_logits_vals = logits[candidate_mask]  # [N]

        # 7. ä¸ºæ¯ä¸ªé«˜ä¼˜å…ˆçº§æ¡¶é€‰ç‚¹
        selected_coords = []

        # éå†æ‰€æœ‰æ¡¶ï¼ˆæŒ‰ç¼ºæŸæ’åºï¼‰ï¼Œç›´åˆ°é€‰å¤Ÿ num ä¸ª
        for bin_idx in sorted_deficit_bins:
            if len(selected_coords) >= num:
                break

            # æ‰¾å€™é€‰ä¸­è½åœ¨è¿™ä¸ªæ¡¶å†…çš„åƒç´ 
            in_bin = (candidate_logits_vals >= bin_edges[bin_idx]) & \
                     (candidate_logits_vals < bin_edges[bin_idx + 1])

            if not in_bin.any():
                continue

            # åœ¨è¯¥æ¡¶å†…ï¼Œé€‰æœ€æ¥è¿‘æ¡¶ä¸­å¿ƒçš„åƒç´ 
            distances = torch.abs(candidate_logits_vals[in_bin] - bin_centers[bin_idx])
            best_local = torch.argmin(distances)
            # æ˜ å°„å›åŸå§‹å€™é€‰ç´¢å¼•
            candidate_in_bin = candidate_indices[in_bin]
            coord = candidate_in_bin[best_local]

            # æ·»åŠ å¹¶ä»å€™é€‰ä¸­ç§»é™¤ï¼ˆé¿å…é‡å¤é€‰ï¼‰
            selected_coords.append(coord)

            # ä»å€™é€‰ä¸­ç§»é™¤è¿™ä¸ªç‚¹ï¼ˆæ›´æ–° candidate_indices å’Œ candidate_logits_valsï¼‰
            mask = (candidate_indices != coord).all(dim=1)
            candidate_indices = candidate_indices[mask]
            candidate_logits_vals = candidate_logits_vals[mask[::]]  # æ³¨æ„ï¼šmask é•¿åº¦=Nï¼Œcandidate_logits_vals é•¿åº¦=N

            # æå‰ç»ˆæ­¢
            if len(selected_coords) == num:
                break

        # å¦‚æœè¿˜æ²¡é€‰å¤Ÿï¼Œç”¨ fallback è¡¥é½
        if len(selected_coords) < num:
            remaining_num = num - len(selected_coords)

            # æ„é€ å½“å‰å·²åŒ…å«çš„ mask_b + å·²é€‰ç‚¹
            temp_mask_b = mask_b.clone()
            if selected_coords:
                selected_tensor = torch.stack(selected_coords)
                temp_mask_b[selected_tensor[:, 0], selected_tensor[:, 1]] = True

            # è°ƒç”¨ fallback å‡½æ•°ï¼ˆç¡®ä¿ä¸é‡å¤é€‰ï¼‰
            fallback_mask = select_uniform_logits_pixels_v2(
                logits, mask_a, temp_mask_b, remaining_num, num_bins=num_bins
            )

            # ä» fallback_mask ä¸­æå–çœŸæ­£æ–°å¢çš„ç‚¹ï¼ˆä¸åœ¨åŸæ¥çš„ mask_b å’Œ selected_coords ä¸­ï¼‰
            combined_so_far = mask_b.clone()
            if selected_coords:
                coords_tensor = torch.stack(selected_coords)
                combined_so_far[coords_tensor[:, 0], coords_tensor[:, 1]] = True

            # æ‰¾å‡º fallback_mask ä¸­æ¯” combined_so_far å¤šå‡ºçš„éƒ¨åˆ†
            new_points_mask = fallback_mask & (~combined_so_far)
            new_points = new_points_mask.nonzero(as_tuple=False)

            # æ·»åŠ æœ€å¤š remaining_num ä¸ª
            add_count = min(remaining_num, new_points.size(0))
            if add_count > 0:
                selected_coords.extend([new_points[i] for i in range(add_count)])

        selected_coords = torch.stack(selected_coords) if selected_coords else torch.empty((0, 2), dtype=torch.long)

    # 8. æ„é€ æœ€ç»ˆ mask
    mask_c = mask_b.clone()
    for i in range(selected_coords.size(0)):
        h, w = selected_coords[i]
        mask_c[h, w] = True

    return mask_c

def get_connected_mask_long_side(mask):
    """
    è¾“å…¥ä¸€ä¸ªå½¢çŠ¶ä¸º [H, W] çš„ PyTorch tensorï¼Œå…¶ä¸­å€¼ä¸º 1 çš„åƒç´ æ„æˆä¸€ä¸ªè¿é€šåŒºåŸŸã€‚
    è¿”å›è¯¥è¿é€šåŒºåŸŸå¤–æ¥çŸ©å½¢çš„é•¿è¾¹é•¿åº¦ã€‚

    å‚æ•°:
        mask (torch.Tensor): shape [H, W], dtype=torch.uint8 or bool or float, å€¼ä¸º 0 æˆ– 1

    è¿”å›:
        int: ç«–è¾¹é•¿åº¦
        int: æ¨ªè¾¹é•¿åº¦
    """
    assert mask.dim() == 2, "mask must be 2D (H, W)"

    # å°† mask è½¬ä¸º bool ç±»å‹
    mask = mask.bool()

    # è·å–æ‰€æœ‰å€¼ä¸º 1 çš„åƒç´ çš„åæ ‡
    coords = torch.nonzero(mask, as_tuple=False)  # å½¢çŠ¶ä¸º [N, 2], æ¯è¡Œæ˜¯ (y, x)

    if coords.numel() == 0:
        return 0  # æ²¡æœ‰å‰æ™¯åƒç´ 

    # æå– x å’Œ y åæ ‡
    y_coords = coords[:, 0]
    x_coords = coords[:, 1]

    # è®¡ç®—è¾¹ç•Œæ¡†
    y_min = y_coords.min().item()
    y_max = y_coords.max().item()
    x_min = x_coords.min().item()
    x_max = x_coords.max().item()

    # è®¡ç®—çŸ©å½¢çš„å®½åº¦å’Œé«˜åº¦
    height = y_max - y_min + 1
    width = x_max - x_min + 1

    # è¿”å›é•¿è¾¹
    return height, width

def compute_weighted_mean_variance(logits: torch.Tensor, mask: torch.Tensor, top_k: int = None):
    """
    è®¡ç®—æ¯ä¸ªåƒç´ åœ¨ mask åŒºåŸŸå†…åŸºäºè·ç¦»åŠ æƒçš„æ–¹å·®ï¼š
    - var_wo: ä¸åŒ…å«å½“å‰åƒç´ ï¼ˆå³ä½¿å®ƒåœ¨ mask ä¸­ï¼‰
    - var_w:  åŒ…å«å½“å‰åƒç´ 
    ä½¿ç”¨ mask ä¸­æœ€è¿‘çš„ top_k ä¸ªç‚¹ï¼Œæ”¯æŒ N < top_k çš„ paddingã€‚
    æƒé‡ä½¿ç”¨æŒ‡æ•°è¡°å‡ï¼Œé¿å…è¿œç‚¹æƒé‡ä¸º0ã€‚
    """
    assert logits.shape == mask.shape
    H, W = logits.shape
    device = logits.device

    # åæ ‡ç½‘æ ¼ (H, W, 2)
    y_coords, x_coords = torch.meshgrid(torch.arange(H, device=device),
                                        torch.arange(W, device=device), indexing='ij')
    coords = torch.stack([y_coords, x_coords], dim=-1).float()  # (H, W, 2)

    # æ‰€æœ‰ mask == 1 çš„ä½ç½®
    mask_positions = mask.nonzero(as_tuple=False)  # (N, 2)
    N = len(mask_positions)
    if N == 0:
        zero = torch.zeros_like(logits)
        return zero, zero, zero, zero

    mask_coords = mask_positions.float()  # (N, 2)
    mask_logits_vals = logits[mask_positions[:, 0], mask_positions[:, 1]]  # (N,)

    # å½“å‰åƒç´ æ˜¯å¦åœ¨ mask ä¸­ï¼Ÿ(H, W)
    self_in_mask = mask.bool()  # (H, W)

    # æ‰€æœ‰ç‚¹åˆ° mask ç‚¹çš„è·ç¦»: (H, W, N)
    all_dists = torch.cdist(coords.view(H*W, 2), mask_coords.unsqueeze(0)).view(H, W, N)

    # === æ­£ç¡®å¤„ç† top_kï¼Œå³ä½¿ k > N ===
    k_pad = top_k if top_k is not None else N
    k_use = min(k_pad, N) if top_k is not None else N

    # 1. å…ˆå¯¹ all_dists åš topkï¼Œé™åˆ¶ k_use <= N
    topk_dists_full, topk_indices_full = torch.topk(all_dists, k_use, dim=-1, largest=False)  # (H, W, k_use)

    # 2. Gather å¯¹åº”çš„ logits
    # mask_logits_vals: (N,)
    topk_logits = torch.gather(
        mask_logits_vals.unsqueeze(0).unsqueeze(0).expand(H, W, N),
        dim=-1,
        index=topk_indices_full
    )  # (H, W, k_use)

    # 3. å¦‚æœéœ€è¦ padding åˆ° k_padï¼Œæ‰è¿›è¡Œ pad
    if k_use < k_pad:
        pad_size = k_pad - k_use
        # Pad distances with inf
        topk_dists = torch.cat([topk_dists_full, topk_dists_full.new_full((H, W, pad_size), float('inf'))], dim=-1)  # (H, W, k_pad)
        # Pad logits with 0 (won't affect due to weight=0)
        topk_logits = torch.cat([topk_logits, topk_logits.new_zeros(H, W, pad_size)], dim=-1)
        # Pad indices: ç”¨ -1 è¡¨ç¤ºæ— æ•ˆï¼Œæˆ–è€…éšä¾¿å¡«ï¼ˆä¸é‡è¦ï¼‰
        topk_indices = torch.cat([topk_indices_full, topk_indices_full.new_full((H, W, pad_size), -1)], dim=-1)
        valid_mask = torch.cat([
            torch.ones_like(topk_indices_full, dtype=torch.bool),
            torch.zeros(H, W, pad_size, dtype=torch.bool, device=device)
        ], dim=-1)
    else:
        topk_dists = topk_dists_full  # (H, W, k_pad)
        topk_indices = topk_indices_full
        valid_mask = torch.ones_like(topk_indices, dtype=torch.bool)  # (H, W, k_pad)

    # åˆ›å»ºä¸€ä¸ªæ˜ å°„ï¼š(y, x) -> index in mask_positions
    # ä½¿ç”¨ coords_to_idx: (H, W)ï¼Œä¸åœ¨ mask ä¸­çš„è®¾ä¸º -1
    coords_to_idx = -torch.ones((H, W), dtype=torch.long, device=device)
    coords_to_idx[mask_positions[:, 0], mask_positions[:, 1]] = torch.arange(N, device=device)

    # å½“å‰åƒç´ åœ¨ mask ä¸­çš„ indexï¼ˆè‹¥ä¸åœ¨åˆ™ä¸º -1ï¼‰
    self_indices = coords_to_idx[y_coords, x_coords]  # (H, W)

    # æ‰©å±•ä¸º (H, W, k_pad)ï¼Œåˆ¤æ–­ topk_indices æ˜¯å¦ç­‰äº self_indices
    self_idx_exp = self_indices.unsqueeze(-1).expand_as(topk_indices)  # (H, W, k_pad)
    is_self = (topk_indices == self_idx_exp)  # (H, W, k_pad), bool

    # åªæœ‰å½“å‰åƒç´ åœ¨ mask ä¸­æ—¶ï¼Œæ‰éœ€è¦å‰”é™¤
    should_del = self_in_mask.unsqueeze(-1).expand_as(is_self)  # (H, W, k_pad)
    is_self_and_valid = is_self & should_del & valid_mask  # ä»…å½“æ˜¯è‡ªèº«ä¸”æœ‰æ•ˆæ—¶æ‰å±è”½

    # === è®¡ç®—æƒé‡ï¼ˆæŒ‡æ•°è¡°å‡ï¼‰===
    # d_max = torch.ones((H, W,k_pad), device=logits.device, dtype=torch.float32) * (H**2 + W**2) ** 0.5 * 0.5    # åŠå¯¹è§’çº¿é•¿åº¦
    d_max = topk_dists.max(dim=-1, keepdim=True).values
    # topk_dists_ = topk_dists.clone()
    # topk_dists_[topk_dists_ == 0] = float('inf')
    d_min = torch.zeros((H, W,k_pad), device=logits.device, dtype=torch.float32)
    eps = 1e-8
    sigma = (d_max - d_min + eps)
    weights = torch.exp(-(topk_dists - d_min) / sigma)  # (H, W, k_pad)
    # print(weights[5,5])

    # å±è”½è‡ªèº«ï¼šå¦‚æœå½“å‰åƒç´ åœ¨ mask ä¸­ä¸”è¢«é€‰å…¥ topkï¼Œåˆ™æƒé‡ç½®0
    weights_masked = weights * (~is_self_and_valid).float()  # å‰”é™¤è‡ªèº«
    float_mask = valid_mask & ~is_self_and_valid
    weights_masked = torch.where(float_mask, weights_masked, torch.zeros_like(weights_masked))  # åŒæ—¶ä¿è¯ pad å’Œ self éƒ½ä¸å‚ä¸
    # print(float_mask[5,5])

    total_weight = weights_masked.sum(dim=-1, keepdim=True)  # (H, W, 1)
    safe_weight = torch.where(total_weight > 0, total_weight, torch.ones_like(total_weight))
    # print(logits[11,8])
    # print(weights_masked[2,6])
    # print(topk_dists[2,6])
    # print(topk_logits[2,6])
    # print(safe_weight[5,5])

    # --- æƒ…å†µ1ï¼šä¸åŒ…å«å½“å‰åƒç´ ï¼ˆå·²å‰”é™¤ï¼‰---
    weighted_sum_wo = (weights_masked * topk_logits).sum(dim=-1)
    mean_wo = weighted_sum_wo / safe_weight.squeeze(-1)
    # print(topk_logits[11,8])
    # print(mean_wo[11,8])
    # print(weighted_sum_wo[5,5])
    # print(mean_wo[5,5])

    mean_exp = mean_wo.unsqueeze(-1)
    var_numerator_wo = (weights_masked * (topk_logits - mean_exp) ** 2).sum(dim=-1)
    var_wo = torch.where(total_weight.squeeze(-1) > 0, var_numerator_wo / safe_weight.squeeze(-1), 0.0)
    # print(var_wo[11,8])

    # --- æƒ…å†µ2ï¼šåŒ…å«å½“å‰åƒç´  ---
    current_logit = logits.unsqueeze(-1)  # (H, W, 1)
    extended_logits = torch.cat([topk_logits, current_logit], dim=-1)  # (H, W, k_pad+1)

    # # å½“å‰åƒç´ è·ç¦»ä¸º0ï¼Œè®¡ç®—å…¶æƒé‡
    # current_d = torch.zeros_like(d_min)
    # current_weight_val = torch.exp(-current_d / sigma)  # (H, W, 1)

    # æ³¨æ„ï¼šå³ä½¿å½“å‰åƒç´ åœ¨ mask ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿ**é‡æ–°åŠ å…¥å®ƒ**ï¼ˆå› ä¸ºè¿™æ˜¯â€œåŒ…å«â€æƒ…å†µï¼‰
    extended_weights = torch.cat([weights_masked, total_weight], dim=-1)  # ç”¨åŸå§‹ weightsï¼ˆæœªå‰”é™¤ï¼‰ï¼Œå†åŠ  current

    # # ä½† extended_weights ä¸­çš„åŸå§‹éƒ¨åˆ†ä»å¯èƒ½åŒ…å«è‡ªèº« â†’ æˆ‘ä»¬å¸Œæœ›åœ¨â€œåŒ…å«â€ä¸­å®ƒæ˜¯ç‹¬ç«‹åŠ å…¥çš„
    # # æ‰€ä»¥ï¼šæˆ‘ä»¬åº”ç¡®ä¿ extended_weights ä¸­åŸå§‹éƒ¨åˆ†ä¸åŒ…å«å½“å‰åƒç´ ï¼ˆé¿å…é‡å¤ï¼‰
    # # æ–¹æ³•ï¼šå°†åŸå§‹ weights ä¸­æŒ‡å‘è‡ªèº«çš„ä¹Ÿç½®0
    # weights_for_ext = weights * (~is_self_and_valid).float() * valid_mask.float()
    # # extended_weights_clean = torch.cat([weights_for_ext, current_weight_val], dim=-1)
    # extended_weights_clean = torch.cat([weights_for_ext, current_weight_val], dim=-1)

    ext_total_weight = extended_weights.sum(dim=-1, keepdim=True)
    safe_ext_weight = torch.where(ext_total_weight > 0, ext_total_weight, torch.ones_like(ext_total_weight))

    mean_w = (extended_weights * extended_logits).sum(dim=-1) / safe_ext_weight.squeeze(-1)
    mean_w_exp = mean_w.unsqueeze(-1)
    # print(mean_w_exp[11,8])
    var_numerator_w = (extended_weights * (extended_logits - mean_w_exp) ** 2).sum(dim=-1)
    var_w = torch.where(ext_total_weight.squeeze(-1) > 0, var_numerator_w / safe_ext_weight.squeeze(-1), 0.0)
    # print(var_w[11,8])

    return mean_wo, var_wo, mean_w, var_w

def compute_weighted_mean_variance_fast_v1(
    logits: torch.Tensor,
    mask_coords: torch.Tensor,        # [N, 2], precomputed
    mask_logits_vals: torch.Tensor,   # [N], precomputed
    coords_grid: torch.Tensor,        # [H*W, 2], precomputed once
    top_k: int = None,
    device: torch.device = None
):
    """
    Fast version for small images (<=64x64).
    Inputs are precomputed sparse representations.
    """
    if mask_coords.numel() == 0:
        H, W = logits.shape
        zero = torch.zeros_like(logits)
        return zero, zero, zero, zero

    H, W = logits.shape
    N = mask_coords.shape[0]
    HW = H * W

    # Flatten logits for easy indexing
    logits_flat = logits.view(-1)  # [HW]

    # All pixel coords: [HW, 2]
    all_pixel_coords = coords_grid  # precomputed

    # Compute distances: [HW, N]
    # For 64x64: HW=4096, N<=4096 â†’ 16M elements, acceptable on GPU
    dists = torch.cdist(all_pixel_coords, mask_coords.float())  # [HW, N]

    k_use = min(top_k, N) if top_k is not None else N
    if k_use == 0:
        zero = torch.zeros_like(logits)
        return zero, zero, zero, zero

    # Top-k nearest (smallest distance)
    topk_dists, topk_indices = torch.topk(dists, k_use, dim=1, largest=False)  # [HW, k_use]

    # Gather logits values for top-k neighbors
    topk_logits = mask_logits_vals[topk_indices]  # [HW, k_use]

    # Determine if current pixel is in mask
    # Build a lookup: pixel_idx -> is_in_mask and its index in mask_coords
    pixel_idx_to_mask_idx = -torch.ones(HW, dtype=torch.long, device=device)
    flat_mask_idx = mask_coords[:, 0] * W + mask_coords[:, 1]  # [N]
    pixel_idx_to_mask_idx[flat_mask_idx] = torch.arange(N, device=device)

    current_pixel_idx = torch.arange(HW, device=device)
    self_in_mask = pixel_idx_to_mask_idx[current_pixel_idx] != -1  # [HW]
    self_mask_idx = pixel_idx_to_mask_idx[current_pixel_idx]       # [HW], -1 if not in mask

    # Expand for comparison
    self_mask_idx_exp = self_mask_idx.unsqueeze(1).expand(-1, k_use)  # [HW, k_use]
    topk_mask_indices = torch.arange(N, device=device)[topk_indices]   # [HW, k_use]
    is_self = (topk_mask_indices == self_mask_idx_exp) & self_in_mask.unsqueeze(1)

    # Compute weights (exponential decay)
    d_max = topk_dists.max(dim=1, keepdim=True).values  # [HW, 1]
    eps = 1e-8
    sigma = d_max + eps
    weights = torch.exp(-topk_dists / sigma)  # [HW, k_use]

    # Mask out self for "without self" case
    weights_wo = weights * (~is_self).float()
    total_weight_wo = weights_wo.sum(dim=1, keepdim=True)  # [HW, 1]
    safe_weight_wo = torch.where(total_weight_wo > 0, total_weight_wo, torch.ones_like(total_weight_wo))

    # Mean without self
    weighted_sum_wo = (weights_wo * topk_logits).sum(dim=1)  # [HW]
    mean_wo = weighted_sum_wo / safe_weight_wo.squeeze(1)

    # Variance without self
    var_numerator_wo = (weights_wo * (topk_logits - mean_wo.unsqueeze(1)) ** 2).sum(dim=1)
    var_wo = torch.where(total_weight_wo.squeeze(1) > 0, var_numerator_wo / safe_weight_wo.squeeze(1), 0.0)

    # For "with self": add current pixel as a new neighbor
    current_logit = logits_flat.unsqueeze(1)  # [HW, 1]
    current_weight = total_weight_wo  # heuristic: use total weight as current weight
    extended_logits = torch.cat([topk_logits, current_logit], dim=1)  # [HW, k_use+1]
    extended_weights = torch.cat([weights_wo, current_weight], dim=1)  # [HW, k_use+1]

    total_weight_w = extended_weights.sum(dim=1, keepdim=True)
    safe_weight_w = torch.where(total_weight_w > 0, total_weight_w, torch.ones_like(total_weight_w))
    mean_w = (extended_weights * extended_logits).sum(dim=1) / safe_weight_w.squeeze(1)
    var_numerator_w = (extended_weights * (extended_logits - mean_w.unsqueeze(1)) ** 2).sum(dim=1)
    var_w = torch.where(total_weight_w.squeeze(1) > 0, var_numerator_w / safe_weight_w.squeeze(1), 0.0)

    # Reshape to [H, W]
    mean_wo = mean_wo.view(H, W)
    var_wo = var_wo.view(H, W)
    mean_w = mean_w.view(H, W)
    var_w = var_w.view(H, W)

    return mean_wo, var_wo, mean_w, var_w

def compute_weighted_mean_variance_fast(
    logits: torch.Tensor,
    mask: torch.Tensor,
    dist_matrix: torch.Tensor,
    coeff: float = 3.0
):
    """
    Compute distance-weighted mean and variance for each pixel using ALL mask points.
    
    Args:
        logits: (H, W)
        mask: (H, W), binary
        dist_matrix: (H*W, H*W), precomputed pairwise distances (flattened coordinates)
    
    Returns:
        mean_wo, var_wo, mean_w, var_w: each (H, W)
    """
    assert logits.shape == mask.shape
    H, W = logits.shape
    HW = H * W
    device = logits.device

    logits_flat = logits.view(HW)               # (HW,)
    mask_flat = mask.view(HW).bool()            # (HW,)
    # dist_matrix = dist_matrix.to(device)  # (HW, HW)

    mask_indices = torch.nonzero(mask_flat, as_tuple=False).squeeze(-1)  # (N,)
    N = mask_indices.numel()

    if N == 0:
        zero = torch.zeros_like(logits)
        return zero, zero, zero, zero

    # Distances from all pixels to mask points
    dists_to_mask = dist_matrix[:, mask_indices]  # (HW, N)
    mask_logits_vals = logits_flat[mask_indices]  # (N,)

    # Compute weights: exp(-d / (max_d + eps))
    d_max = dists_to_mask.max(dim=1, keepdim=True).values  # (HW, 1)
    eps = 1e-8
    weights_full = torch.exp(- coeff * dists_to_mask / (d_max + eps))  # (HW, N)

    # Identify if current pixel is in mask and appears in mask_indices
    self_in_mask = mask_flat  # (HW,)
    is_self = (mask_indices.unsqueeze(0) == torch.arange(HW, device=device).unsqueeze(1))  # (HW, N)
    is_self_and_valid = is_self & self_in_mask.unsqueeze(1)  # (HW, N)

    # --- Without self ---
    weights_wo = weights_full * (~is_self_and_valid).float()  # (HW, N)

    weight_sum_wo = weights_wo.sum(dim=1)  # (HW,)
    has_valid_wo = weight_sum_wo > 0

    # print(weights_wo[0]/weight_sum_wo[0])
    # # print(weighted_sum_wo[0])
    # a = input()

    # Compute mean_wo safely
    weighted_sum_wo = (weights_wo * mask_logits_vals.unsqueeze(0)).sum(dim=1)  # (HW,)
    mean_wo = torch.zeros_like(weighted_sum_wo)
    mean_wo[has_valid_wo] = weighted_sum_wo[has_valid_wo] / weight_sum_wo[has_valid_wo]

    # Compute var_wo
    var_wo = torch.zeros_like(weighted_sum_wo)
    if has_valid_wo.any():
        diff = mask_logits_vals.unsqueeze(0) - mean_wo.unsqueeze(1)  # (HW, N)
        var_num = (weights_wo * (diff ** 2)).sum(dim=1)  # (HW,)
        var_wo[has_valid_wo] = var_num[has_valid_wo] / weight_sum_wo[has_valid_wo]

    # --- With self: add current pixel with weight = weight_sum_wo (or 1 if no neighbors) ---
    current_logit = logits_flat  # (HW,)

    # è‡ªèº«æƒé‡è®¾è®¡ï¼šè‹¥å‘¨å›´æ— ç‚¹ï¼Œåˆ™ w_self = 1ï¼›å¦åˆ™ w_self = weight_sum_wo
    # è¿™æ ·ä¿è¯è‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆæƒé‡
    weight_self = torch.where(has_valid_wo, weight_sum_wo, torch.ones_like(weight_sum_wo))  # (HW,)

    # Total weight for "with self"
    weight_sum_w = weight_sum_wo + weight_self  # (HW,)

    # Mean_w = (W_wo * mean_wo + w_self * x_self) / (W_wo + w_self)
    mean_w = (weight_sum_wo * mean_wo + weight_self * current_logit) / weight_sum_w

    # Variance_w:
    # Var = [W_wo*(var_wo + (mean_wo - mean_w)^2) + w_self*(0 + (x_self - mean_w)^2)] / (W_wo + w_self)
    term1 = weight_sum_wo * (var_wo + (mean_wo - mean_w) ** 2)
    term2 = weight_self * (current_logit - mean_w) ** 2
    var_w = (term1 + term2) / weight_sum_w

    # Reshape
    return mean_wo.view(H, W), var_wo.view(H, W), mean_w.view(H, W), var_w.view(H, W)

def compute_weighted_mean_variance_fast_v3(
    logits: torch.Tensor,
    mask: torch.Tensor,
    dist_matrix: torch.Tensor,
    top_k: int = 128,  # reasonable default
    coeff: float = 1.0
):
    """
    Efficient GPU implementation with top-k and precomputed distance matrix.
    Assumes logits is top-left (H, W) of 64x64 image.
    """
    assert logits.shape == mask.shape
    H, W = logits.shape
    HW = H * W
    device = logits.device

    logits_flat = logits.view(HW)
    mask_flat = mask.view(HW).bool()

    mask_indices = torch.nonzero(mask_flat, as_tuple=False).squeeze(-1)
    N = mask_indices.numel()

    if N == 0:
        zero = torch.zeros_like(logits)
        return zero, zero, zero, zero

    # Limit top_k to avoid OOM
    k_use = min(top_k, N)

    # --- Get top-k nearest mask points for each pixel ---
    # dists: (HW, N)
    dists_to_mask = dist_matrix[:, mask_indices]
    topk_dists, topk_idx_in_mask = torch.topk(dists_to_mask, k_use, dim=1, largest=False)  # (HW, k_use)

    # Gather logits of top-k points
    topk_logits = logits_flat[mask_indices[topk_idx_in_mask]]  # (HW, k_use)

    # --- Compute weights (exponential decay) ---
    d_max = topk_dists.max(dim=1, keepdim=True).values  # (HW, 1)
    eps = 1e-8
    weights = torch.exp(-coeff * topk_dists / (d_max + eps))  # (HW, k_use)

    # --- Identify if current pixel is in the top-k and in mask ---
    # Global index of current pixel: 0,1,...,HW-1
    current_global_idx = torch.arange(HW, device=device)  # (HW,)
    # Global index of top-k mask points:
    topk_global_idx = mask_indices[topk_idx_in_mask]  # (HW, k_use)
    # Is current pixel in mask?
    self_in_mask = mask_flat  # (HW,)
    # Is current pixel among the top-k neighbors?
    is_self_in_topk = (topk_global_idx == current_global_idx.unsqueeze(1))  # (HW, k_use)
    # Only mask out if it's both in mask AND in top-k
    should_mask_self = self_in_mask.unsqueeze(1) & is_self_in_topk  # (HW, k_use)

    # --- Case 1: Without self ---
    weights_wo = torch.where(should_mask_self, torch.zeros_like(weights), weights)
    weight_sum_wo = weights_wo.sum(dim=1)  # (HW,)
    has_valid_wo = weight_sum_wo > 0

    # Compute mean_wo
    weighted_sum_wo = (weights_wo * topk_logits).sum(dim=1)
    mean_wo = torch.zeros_like(weighted_sum_wo)
    mean_wo[has_valid_wo] = weighted_sum_wo[has_valid_wo] / weight_sum_wo[has_valid_wo]

    # Compute var_wo
    var_wo = torch.zeros_like(weighted_sum_wo)
    if has_valid_wo.any():
        diff = topk_logits - mean_wo.unsqueeze(1)
        var_num = (weights_wo * (diff ** 2)).sum(dim=1)
        var_wo[has_valid_wo] = var_num[has_valid_wo] / weight_sum_wo[has_valid_wo]

    # --- Case 2: With self ---
    current_logit = logits_flat
    # Self weight: use total weight of neighbors (or 1 if none)
    weight_self = torch.where(has_valid_wo, weight_sum_wo, torch.ones_like(weight_sum_wo))
    weight_sum_w = weight_sum_wo + weight_self

    mean_w = (weight_sum_wo * mean_wo + weight_self * current_logit) / weight_sum_w

    # Incremental variance
    term1 = weight_sum_wo * (var_wo + (mean_wo - mean_w) ** 2)
    term2 = weight_self * (current_logit - mean_w) ** 2
    var_w = (term1 + term2) / weight_sum_w

    return (
        mean_wo.view(H, W),
        var_wo.view(H, W),
        mean_w.view(H, W),
        var_w.view(H, W)
    )

def keep_negative_by_top2_magnitude_levels_old(x, target_size):
    """
    åœ¨ tensor ä¸­ï¼Œå¯¹è´Ÿæ•°éƒ¨åˆ†ï¼š
    - è®¡ç®—æ¯ä¸ªè´Ÿæ•°çš„åè¿›åˆ¶æ•°é‡çº§ï¼ˆfloor(log10(|x|))ï¼‰
    - æ‰¾å‡ºæœ€å¤§çš„ä¸¤ä¸ªæ•°é‡çº§
    - ä¿ç•™å±äºè¿™ä¸¤ä¸ªæ•°é‡çº§çš„è´Ÿæ•°ï¼Œå…¶ä½™è´Ÿæ•°ç½®ä¸º 0
    - éè´Ÿæ•°ä¿æŒä¸å˜
    Args:
        x (torch.Tensor): è¾“å…¥ tensor

    Returns:
        torch.Tensor: å¤„ç†åçš„ tensor
    """
    out = x.clone()

    # æ‰¾å‡ºè´Ÿæ•°
    negative_mask = x < 0
    if not negative_mask.any():
        return out  # æ²¡æœ‰è´Ÿæ•°ï¼Œç›´æ¥è¿”å›

    negative_vals = x[negative_mask]

    # è®¡ç®—ç»å¯¹å€¼
    abs_vals = negative_vals.abs()
    if negative_mask.float().sum() / target_size > 2:
        robust_max_idx = int(np.ceil(target_size * (1 - 0.8)))
        sorted_abs_vals = torch.sort(abs_vals.view(-1), descending=True).values
        robust_max = sorted_abs_vals[robust_max_idx]
    else:
        robust_max = torch.quantile(abs_vals, 0.90)
    abs_vals = torch.clamp_max(abs_vals, max=robust_max)
    # print(robust_max)

    # å®‰å…¨å¤„ç†ï¼šé¿å… log10(0)ï¼Œä½†è´Ÿæ•°ç»å¯¹å€¼ä¸ä¼šä¸º0
    # è®¡ç®—æ•°é‡çº§ï¼šfloor(log10(abs(x)))
    magnitudes = torch.floor(torch.log10(abs_vals)).to(torch.int)

    # å¦‚æœåªæœ‰ä¸€ä¸ªè´Ÿæ•°ï¼Œç›´æ¥ä¿ç•™
    if len(magnitudes) <= 2:
        return out

    # æ‰¾å‡ºå”¯äºŒçš„æ•°é‡çº§ï¼Œæ’åºï¼ˆä»å¤§åˆ°å°ï¼‰
    unique_magnitudes = torch.unique(magnitudes, sorted=True)  # å‡åº
    top2_magnitudes = unique_magnitudes[-2:]  # æœ€å¤§çš„ä¸¤ä¸ªæ•°é‡çº§

    # æ‰¾å‡ºå“ªäº›è´Ÿæ•°å±äºè¿™ä¸¤ä¸ªæ•°é‡çº§
    keep_negative = (magnitudes.unsqueeze(1) == top2_magnitudes.unsqueeze(0)).any(dim=1)

    # æ˜ å°„å›åŸ tensor çš„ç´¢å¼•
    negative_indices = torch.nonzero(negative_mask, as_tuple=False).squeeze(1)
    keep_in_negative = negative_indices[keep_negative]

    # æ„å»ºæœ€ç»ˆä¿ç•™ maskï¼šéè´Ÿæ•° + å±äº top2 æ•°é‡çº§çš„è´Ÿæ•°
    final_keep_mask = torch.zeros_like(x, dtype=torch.bool)
    final_keep_mask[keep_in_negative[:, 0], keep_in_negative[:,1]] = True
    final_keep_mask |= (x > 0)  # åŠ ä¸Šæ­£æ•°

    return out * final_keep_mask

def keep_negative_by_top2_magnitude_levels_old2(x, target_size):
    """
    åœ¨ tensor ä¸­ï¼Œå¯¹è´Ÿæ•°éƒ¨åˆ†ï¼š
    - è®¡ç®—æ¯ä¸ªè´Ÿæ•°çš„åè¿›åˆ¶æ•°é‡çº§ï¼ˆfloor(log10(|x|))ï¼‰
    - è®¡ç®—é²æ£’æœ€å¤§å€¼ robust_maxï¼ˆåŸºäº target_size çš„é²æ£’æˆªæ–­æˆ–90%åˆ†ä½ï¼‰
    - å– robust_max å¯¹åº”çš„æ•°é‡çº§ level_max = floor(log10(robust_max))
    - ä¿ç•™å±äº level_max å’Œ level_max - 1 çš„è´Ÿæ•°ï¼Œå…¶ä½™è´Ÿæ•°ç½®ä¸º 0
    - éè´Ÿæ•°ä¿æŒä¸å˜

    Args:
        x (torch.Tensor): è¾“å…¥ tensor
        target_size (int): ç”¨äºè®¡ç®—é²æ£’æœ€å¤§å€¼çš„å‚è€ƒå°ºå¯¸ï¼ˆå¦‚æ€»å…ƒç´ æ•°ï¼‰

    Returns:
        torch.Tensor: å¤„ç†åçš„ tensor
    """
    out = x.clone()

    # æ‰¾å‡ºè´Ÿæ•°
    negative_mask = x < 0
    if not negative_mask.any():
        return out  # æ²¡æœ‰è´Ÿæ•°ï¼Œç›´æ¥è¿”å›

    negative_vals = x[negative_mask]
    abs_vals = negative_vals.abs()

    # --- è®¡ç®—é²æ£’æœ€å¤§å€¼ ---
    if negative_mask.float().sum() / target_size > 2:
        robust_max_idx = torch.ceil(target_size * (1 - 0.8)).int()
        sorted_abs_vals = torch.sort(abs_vals.view(-1), descending=True).values
        robust_max = sorted_abs_vals[robust_max_idx] if robust_max_idx < len(sorted_abs_vals) else sorted_abs_vals[-1]
    else:
        robust_max = torch.quantile(abs_vals, 0.90)

    # å®‰å…¨æˆªæ–­ï¼šé¿å…å¼‚å¸¸å¤§å€¼å½±å“æ•°é‡çº§è®¡ç®—
    abs_vals = torch.clamp_max(abs_vals, max=robust_max)

    # --- è®¡ç®—é²æ£’æœ€å¤§å€¼å¯¹åº”çš„æ•°é‡çº§ ---
    # æ³¨æ„ï¼šrobust_max å¯èƒ½ä¸º0ï¼Ÿä½†è´Ÿæ•°ç»å¯¹å€¼>0ï¼Œæ‰€ä»¥å®‰å…¨
    level_max = torch.floor(torch.log10(robust_max)).to(torch.int).item()

    # å®šä¹‰è¦ä¿ç•™çš„æ•°é‡çº§ï¼šlevel_max å’Œ level_max - 1
    keep_levels = {level_max, level_max - 1}

    # --- è®¡ç®—æ¯ä¸ªè´Ÿæ•°çš„æ•°é‡çº§ ---
    magnitudes = torch.floor(torch.log10(abs_vals)).to(torch.int)

    # --- æ‰¾å‡ºå“ªäº›è´Ÿæ•°å±äº keep_levels ---
    keep_negative = torch.isin(magnitudes, torch.tensor(list(keep_levels), dtype=magnitudes.dtype, device=magnitudes.device))

    # --- æ˜ å°„å›åŸ tensor çš„ç´¢å¼• ---
    negative_indices = torch.nonzero(negative_mask, as_tuple=False).squeeze(1)
    keep_in_negative = negative_indices[keep_negative]

    # --- æ„å»ºæœ€ç»ˆä¿ç•™ mask ---
    final_keep_mask = torch.zeros_like(x, dtype=torch.bool)

    # å¦‚æœ keep_in_negative æ˜¯ä¸€ç»´çš„ï¼ˆå•é€šé“ï¼‰ï¼Œéœ€è¦å¤„ç†ç»´åº¦
    if keep_in_negative.dim() == 1:
        final_keep_mask[keep_in_negative] = True
    else:
        final_keep_mask[keep_in_negative[:, 0], keep_in_negative[:, 1]] = True

    # åŠ ä¸Šéè´Ÿæ•°ï¼ˆ>0 çš„éƒ¨åˆ†ï¼‰
    final_keep_mask |= (x >= 0)  # æ³¨æ„ï¼šè¿™é‡Œåº”è¯¥æ˜¯ >=0ï¼Œå› ä¸º0ä¹Ÿä¿ç•™ï¼ˆåŸé€»è¾‘æ˜¯>0ï¼Œä½†0ä¸æ˜¯è´Ÿæ•°ï¼Œä¹Ÿåº”è¯¥ä¿ç•™ï¼‰

    return out * final_keep_mask

def keep_negative_by_top2_magnitude_levels(x, target_size):
    out = x.clone()
    negative_mask = x < 0
    if not negative_mask.any():
        return out

    negative_vals = x[negative_mask]
    abs_vals = negative_vals.abs()

    # --- é²æ£’æœ€å¤§å€¼ ---
    neg_count = negative_mask.sum().float()
    if neg_count / target_size > 2:
        robust_max_idx = torch.ceil(target_size * 0.2).long()
        sorted_abs = torch.sort(abs_vals, descending=True).values
        robust_max = sorted_abs[min(robust_max_idx, len(sorted_abs) - 1)]
    else:
        robust_max = torch.quantile(abs_vals, 0.9)

    abs_vals = torch.clamp_max(abs_vals, robust_max)

    # --- è®¡ç®—æ•°é‡çº§ï¼ˆä¿æŒä¸ºå¼ é‡ï¼‰---
    log10_abs = torch.log10(abs_vals + 1e-12)  # é˜²æ­¢ log(0)
    magnitudes = torch.floor(log10_abs).long()  # [M]

    # --- è®¡ç®— level_maxï¼ˆå¼ é‡ï¼‰---
    level_max = torch.floor(torch.log10(robust_max + 1e-12)).long()

    # --- æ„å»ºä¿ç•™æ©ç ï¼ˆå‘é‡åŒ–ï¼‰---
    keep_mask_mag = (magnitudes == level_max) | (magnitudes == level_max - 1)

    # --- æ˜ å°„å›åŸå›¾ ---
    final_keep = torch.zeros_like(x, dtype=torch.bool)
    neg_indices = torch.nonzero(negative_mask, as_tuple=False)  # [M, 2]
    if keep_mask_mag.any():
        keep_indices = neg_indices[keep_mask_mag]
        final_keep[keep_indices[:, 0], keep_indices[:, 1]] = True

    final_keep |= (x >= 0)
    return out * final_keep

def smooth_optim(logits, pos_bias=0.5):
    H, W = logits.shape
    # å…ˆå¯¹logitsè¿›è¡Œæ˜ å°„
    credit_pos = (logits < 0.) * (-logits)
    credit_neg = (logits > 0.) * logits
    credit_pos_ = -(credit_pos / (credit_pos.max() + 1e-8) * (1 - pos_bias) + pos_bias * (logits < 0.))
    credit_neg_ = credit_neg / (credit_neg.max()+ 1e-8)
    credit = credit_pos_ + credit_neg_  #(H, W)

    padded_credit = F.pad(credit.unsqueeze(0).unsqueeze(0), (1, 1, 1, 1), mode='replicate')
    smooth_credit = gaussian_blurring_2D(padded_credit, 3, sigma=1)[1:1+H, 1:1+W]   

    smooth_credit_pos = smooth_credit * (smooth_credit < 0.0)
    smooth_credit_neg = smooth_credit * (smooth_credit > 0.0)
    smooth_credit_pos_ = (smooth_credit_pos - pos_bias * (logits < 0.)) / (1 - pos_bias) * credit_pos.max()
    smooth_credit_neg_ = smooth_credit_neg * credit_neg.max()
    new_logits = smooth_credit_pos_ + smooth_credit_neg_

    fig = plt.figure(figsize=(25, 5))
    plt.subplot(1, 5, 1)
    plt.imshow(logits, cmap='gray')
    plt.subplot(1, 5, 2)
    plt.imshow(credit, cmap='gray')
    plt.subplot(1, 5, 3)
    plt.imshow(smooth_credit, cmap='gray')
    plt.show(block=False)
    plt.subplot(1, 5, 4)
    plt.imshow(smooth_credit_pos_, cmap='gray')
    plt.subplot(1, 5, 5)
    plt.imshow(new_logits, cmap='gray')
    a = input()

    return new_logits

def add_uniform_points_cuda(mask, seed, num1, part_ratio=1, logits=None, chunk_size=2048):
    """
    CUDA-friendly version of adding uniformly distributed points.
    Uses chunked distance computation to avoid OOM and maximize GPU utilization.

    Args:
        mask (torch.Tensor): [H, W], bool, valid region
        seed (torch.Tensor): [H, W], bool, existing points
        num1 (int): number of new points to add
        part_ratio (float): ratio of points to add to the part of the image
        logits (torch.Tensor): [H, W], float, logits to select points part
        chunk_size (int): chunk size for distance computation to control memory

    Returns:
        new_seed (torch.Tensor): updated seed map with added points
    """
    assert mask.shape == seed.shape
    assert mask.dim() == 2
    device = mask.device
    dtype = torch.float32

    H, W = mask.shape

    # Convert to bool
    mask = mask.bool()
    seed = seed.bool()

    # Existing points: (N, 2) format (y, x)
    existing_points = torch.nonzero(seed, as_tuple=False).float()  # [N, 2]
    N = existing_points.shape[0]

    # Candidate points: in mask but not in seed
    candidate_mask = mask & (~seed)
    candidates = torch.nonzero(candidate_mask, as_tuple=False).float()  # [M, 2]
    M = candidates.shape[0]

    if num1 <= 0:
        return seed.clone()
    if M == 0:
        return seed.clone()
    if M < num1:
        num1 = M

    # Move to GPU if not already
    candidates = candidates.to(device, non_blocking=True)
    existing_points = existing_points.to(device, non_blocking=True)

    new_seed = torch.zeros_like(seed)
    added = 0

    # Pre-allocate for added points (for FPS-style update)
    all_selected = existing_points  # Will grow, but we avoid cat in loop if possible

    for _ in range(num1):
        if candidates.shape[0] == 0:
            break

        # Compute min distance from each candidate to all_selected
        min_dists = torch.full((candidates.shape[0],), float('inf'), device=device, dtype=dtype)

        # Chunked distance computation to avoid memory overflow
        for i in range(0, all_selected.shape[0], chunk_size):
            batch = all_selected[i:i+chunk_size]  # [B, 2]
            # Compute distance: candidates [M', 2] vs batch [B, 2]
            dists = torch.cdist(candidates, batch)  # [M', B]
            min_dists = torch.min(min_dists, torch.min(dists, dim=1).values)  # update min

        # Find candidate with maximum of min distances
        if min_dists.numel() == 0:
            break
        idx = torch.argmax(min_dists)

        # Selected point (in float format)
        selected_float = candidates[idx]  # [2], (y, x)
        selected_int = selected_float.round().long()
        y, x = selected_int[0].item(), selected_int[1].item()

        # Update new_seed
        new_seed[y, x] = True
        added += 1

        # Add to all_selected
        all_selected = torch.cat([all_selected, selected_float.unsqueeze(0)], dim=0)

        # Remove selected from candidates
        mask_out = torch.any(candidates != selected_float, dim=1)
        candidates = candidates[mask_out]

    updated_seed = seed.clone()

    if part_ratio < 1:
        candidates_seed_logits = logits * new_seed
        k = int(num1 * part_ratio)
        k = k if k > 0 else 1
        _, idx = torch.topk(candidates_seed_logits.view(-1), k)
        for j in idx:
            updated_seed[j // W, j % W] = True
    else:
        updated_seed = updated_seed | new_seed

    return updated_seed

def add_uniform_points_grid_cuda_v1(mask, seed, num, jitter=True):
    """
    Add 'num' uniformly distributed points inside mask, avoiding seed,
    using jittered grid + farthest selection (GPU optimized).

    Args:
        mask (torch.Tensor): [H, W], bool, valid region
        seed (torch.Tensor): [H, W], bool, existing points
        num (int): number of new points to add
        jitter (bool): whether to add random jitter in each grid cell

    Returns:
        torch.Tensor: updated seed map, same shape as input
    """
    device = mask.device
    H, W = mask.shape

    # Ensure boolean
    mask = mask.bool()
    seed = seed.bool()

    # Existing seed points (float format for distance computation)
    existing_points = torch.nonzero(seed, as_tuple=False).float()  # [N, 2] (y, x)
    N = existing_points.shape[0]

    # Candidate area: mask but not seed
    candidate_mask = mask & (~seed)
    if num <= 0:
        return seed.clone()
    if not candidate_mask.any():
        return seed.clone()

    # -------------------------------
    # Step 1: Generate jittered grid candidates
    # -------------------------------
    n_side = int(num ** 0.5) + 2  # Slightly over-sample
    gy = torch.linspace(0, H - 1e-5, n_side + 1, device=device)  # Avoid edge issues
    gx = torch.linspace(0, W - 1e-5, n_side + 1, device=device)

    # æ›¿æ¢æ•´ä¸ª grid ç”Ÿæˆéƒ¨åˆ†
    gy = torch.linspace(0, H - 1e-5, n_side + 1, device=device)
    gx = torch.linspace(0, W - 1e-5, n_side + 1, device=device)

    # ç”Ÿæˆç½‘æ ¼åæ ‡ (n_side, n_side)
    gy_mid = (gy[:-1] + gy[1:]) / 2  # [n_side]
    gx_mid = (gx[:-1] + gx[1:]) / 2  # [n_side]

    # meshgrid
    Y, X = torch.meshgrid(gy_mid, gx_mid, indexing='ij')  # [n_side, n_side]
    candidates_y = Y.flatten()  # [K]
    candidates_x = X.flatten()  # [K]

    if jitter:
        dy = torch.rand_like(candidates_y) * (gy[1] - gy[0])
        dx = torch.rand_like(candidates_x) * (gx[1] - gx[0])
        candidates_y += dy
        candidates_x += dx

    candidates = torch.stack([candidates_y, candidates_x], dim=1)  # [K, 2]
    K = candidates.shape[0]

    # Convert to integer coordinates for indexing
    cand_int = candidates.long()  # [K, 2] (y, x)

    # Filter: within bounds
    valid = (cand_int[:, 0] < H) & (cand_int[:, 1] < W) & (cand_int[:, 0] >= 0) & (cand_int[:, 1] >= 0)
    candidates = candidates[valid]
    cand_int = cand_int[valid]

    # Further filter: must be in mask and not in seed
    spatial_valid = mask[cand_int[:, 0], cand_int[:, 1]] & (~seed[cand_int[:, 0], cand_int[:, 1]])
    candidates = candidates[spatial_valid]
    cand_int = cand_int[spatial_valid]

    # If not enough grid points, supplement with random samples from candidate_mask
    current_num = candidates.shape[0]
    if current_num < num:
        extra_coords = torch.nonzero(candidate_mask, as_tuple=False).float()
        if extra_coords.shape[0] > 0:
            # Remove duplicates (based on int coords)
            all_existing_int = torch.cat([
                torch.nonzero(seed, as_tuple=False),
                cand_int
            ], dim=0)
            # Use set logic via unique
            grid_or_seed = torch.zeros_like(candidate_mask)
            for y, x in torch.unique(all_existing_int, dim=0):
                if 0 <= y < H and 0 <= x < W:
                    grid_or_seed[y, x] = True
            extra_candidate_mask = candidate_mask & (~grid_or_seed)
            extra_coords = torch.nonzero(extra_candidate_mask, as_tuple=False).float()

            if extra_coords.shape[0] > 0:
                # Randomly sample up to (num - current_num)
                n_extra = min(num - current_num, extra_coords.shape[0])
                perm = torch.randperm(extra_coords.shape[0], device=device)[:n_extra]
                extra_candidates = extra_coords[perm]
                candidates = torch.cat([candidates, extra_candidates], dim=0)
                cand_int = torch.cat([cand_int, extra_candidates.long()], dim=0)

    # Now we have at least `num` candidates (or fewer if very constrained)
    if candidates.shape[0] == 0:
        return seed.clone()

    # -------------------------------
    # Step 2: Select top-`num` points that are farthest from existing seed
    # -------------------------------
    if N > 0 and candidates.shape[0] > 0:
        # Compute min distance from each candidate to any existing point
        dists = torch.cdist(candidates, existing_points)  # [K', N]
        min_dists = dists.min(dim=1).values  # [K']
        # Select the `num` points with largest min-distance
        k_select = min(num, min_dists.shape[0])
        _, idx_selected = torch.topk(min_dists, k=k_select, largest=True)
    else:
        # No existing points or no candidates -> random selection
        k_select = min(num, candidates.shape[0])
        idx_selected = torch.randperm(candidates.shape[0], device=device)[:k_select]

    final_cand_int = cand_int[idx_selected]  # [k_select, 2]

    # -------------------------------
    # Step 3: Update seed map
    # -------------------------------
    new_seed = seed.clone()
    # Use advanced indexing for batch write (efficient on GPU)
    ys = final_cand_int[:, 0]
    xs = final_cand_int[:, 1]
    valid_update = (ys < H) & (xs < W) & (ys >= 0) & (xs >= 0)
    ys = ys[valid_update]
    xs = xs[valid_update]
    new_seed[ys, xs] = True

    return new_seed

def add_uniform_points_grid_cuda(mask, seed, num, min_dist_ratio=0.5):
    """
    add_uniform_points_nms
    Add 'num' approximately uniformly distributed points in mask (excluding seed)
    using random sampling + distance-based Non-Maximum Suppression (NMS).
    
    This approximates Poisson disk sampling by ensuring selected points are not too close.
    
    Args:
        mask (torch.Tensor): [H, W], bool, valid region
        seed (torch.Tensor): [H, W], bool, existing points
        num (int): number of new points to add
        min_dist_ratio (float): controls min distance as ratio of avg grid spacing.
                                Smaller -> denser; larger -> sparser.
                                Typical: 0.3 ~ 0.7.

    Returns:
        torch.Tensor: updated seed map [H, W], bool
    """
    if num <= 0:
        return seed.clone()
    
    mask = mask.bool()
    seed = seed.bool()
    H, W = mask.shape
    device = mask.device

    candidate_mask = mask & (~seed)
    candidate_coords = torch.nonzero(candidate_mask, as_tuple=False).float()  # [M, 2]
    M = candidate_coords.shape[0]

    if M == 0:
        return seed.clone()
    
    # If not enough candidates, take all
    if M <= num:
        selected = candidate_coords
    else:
        # Step 1: Oversample candidates (e.g., 3x~5x)
        oversample = min(5 * num, M)
        idx = torch.randperm(M, device=device)[:oversample]
        candidates = candidate_coords[idx]  # [K, 2], K = oversample
        
        # Step 2: Estimate a reasonable min distance
        # Approximate average spacing if points were uniform over mask area
        mask_area = mask.sum().float()
        if mask_area > 0:
            avg_spacing = (mask_area ** 0.5) / (num ** 0.5 + 1e-6)
        else:
            avg_spacing = min(H, W) * 0.1
        min_dist = min_dist_ratio * avg_spacing

        # Step 3: Distance-based NMS
        # Start with all candidates as potential
        keep = torch.ones(candidates.shape[0], dtype=torch.bool, device=device)
        selected = []

        # Compute pairwise distances once (K can be ~500, so K^2 is acceptable)
        dists = torch.cdist(candidates, candidates)  # [K, K]

        # Greedily select: pick highest-score (here, all equal) and suppress neighbors
        # Since no score, we just iterate in order (or shuffle for randomness)
        order = torch.randperm(candidates.shape[0], device=device)
        for i in order:
            if not keep[i]:
                continue
            selected.append(candidates[i:i+1])
            if len(selected) >= num:
                break
            # Suppress all points within min_dist
            too_close = dists[i] < min_dist
            keep = keep & (~too_close)

        if selected:
            selected = torch.cat(selected, dim=0)
        else:
            # Fallback: just take first 'num'
            selected = candidates[:num]

    # Update seed map
    new_seed = seed.clone()
    ys = selected[:, 0].long()
    xs = selected[:, 1].long()
    valid = (ys >= 0) & (ys < H) & (xs >= 0) & (xs < W)
    new_seed[ys[valid], xs[valid]] = True
    return new_seed

def add_uniform_points_with_logits(mask, seed, logits, num, alpha=0.6, jitter=True):
    """
    Add 'num' points in mask region using adaptive grid based on mask area.
    Generates ~2*num candidates to handle irregular shapes, then selects top-num by score.

    Args:
        mask (torch.Tensor): [H, W], bool
        seed (torch.Tensor): [H, W], bool
        logits (torch.Tensor): [H, W], float
        num (int): number of points to add
        alpha (float): weight for spatial uniformity (0~1)
        jitter (bool): whether to add random jitter within each cell

    Returns:
        torch.Tensor: updated seed map
    """
    device = mask.device
    H, W = mask.shape

    assert mask.shape == seed.shape == logits.shape
    assert 0 <= alpha <= 1
    if num <= 0:
        return seed.clone()

    # -------------------------------
    # Step 0: Prepare candidate area
    # -------------------------------
    mask = mask.bool()
    seed = seed.bool()
    candidate_mask = mask & (~seed)

    if not candidate_mask.any():
        return seed.clone()

    # -------------------------------
    # Step 1: Compute adaptive grid cell size from mask area
    # -------------------------------
    area = mask.sum().float().item()
    if area < 1e-3:
        num = torch.ceil(area).int()

    # Estimate ideal cell size: sqrt(area / num)
    cell_size = (area / num) ** 0.5
    cell_size = max(1.0, cell_size)  # At least 1x1

    # Number of cells in H and W direction
    nx = int(W / cell_size) + 2
    ny = int(H / cell_size) + 2

    # Grid boundaries
    x_edges = torch.linspace(0, W, nx + 1, device=device)
    y_edges = torch.linspace(0, H, ny + 1, device=device)

    candidates = []

    # æ›¿æ¢åŸæ¥çš„ for å¾ªç¯
    y_centers = (y_edges[:-1] + y_edges[1:]) / 2  # [ny]
    x_centers = (x_edges[:-1] + x_edges[1:]) / 2  # [nx]

    if jitter:
        y_jitter = (torch.rand_like(y_centers) * cell_size)
        x_jitter = (torch.rand_like(x_centers) * cell_size)
        y_centers = y_centers + y_jitter
        x_centers = x_centers + x_jitter

    # æ‰©å±•ä¸ºç½‘æ ¼ç‚¹
    yy, xx = torch.meshgrid(y_centers, x_centers, indexing='ij')  # [ny, nx]
    candidates = torch.stack([yy.flatten(), xx.flatten()], dim=1)  # [K, 2]

    # Filter valid indices
    cand_int = candidates.long()  # [K, 2] (y, x)
    valid_pos = (cand_int[:, 0] < H) & (cand_int[:, 1] < W) & (cand_int[:, 0] >= 0) & (cand_int[:, 1] >= 0)
    candidates = candidates[valid_pos]
    cand_int = cand_int[valid_pos]

    # Further filter: must be in candidate_mask
    ys, xs = cand_int[:, 0], cand_int[:, 1]
    in_candidate = candidate_mask[ys, xs]
    candidates = candidates[in_candidate]
    cand_int = cand_int[in_candidate]
    logits_vals = logits[ys, xs][in_candidate]

    if candidates.shape[0] == 0:
        return seed.clone()

    # -------------------------------
    # Step 2: Oversample strategy â€” keep up to 2*num candidates
    # -------------------------------
    K = candidates.shape[0]
    target_keep = min(K, 2 * num)  # Aim for ~2*num

    # Optional: sort by spatial spread or logits before truncating?
    # We'll keep them as-is (random order due to grid), then re-rank by score later

    if K > target_keep:
        # Randomly subsample to avoid OOM in distance computation
        perm = torch.randperm(K, device=device)[:target_keep]
        candidates = candidates[perm]
        cand_int = cand_int[perm]
        logits_vals = logits_vals[perm]

    # -------------------------------
    # Step 3: Compute combined score
    # -------------------------------
    scores = torch.zeros(candidates.shape[0], device=device, dtype=torch.float32)

    # Term 1: Distance to nearest existing seed (if any)
    has_existing = seed.any()
    if has_existing and alpha > 0.0:
        existing_points = torch.nonzero(seed, as_tuple=False).float()
        dists = torch.cdist(candidates, existing_points)
        min_dists = dists.min(dim=1).values
        max_dist = min_dists.max() + 1e-8
        norm_dist = min_dists / max_dist
        scores += alpha * norm_dist

    # Term 2: Logits importance
    logit_score = torch.sigmoid(logits_vals)  # Normalize to [0,1]
    scores += (1 - alpha) * logit_score

    # -------------------------------
    # Step 4: Select top-`num` points
    # -------------------------------
    k_select = min(num, scores.shape[0])
    _, idx_selected = torch.topk(scores, k=k_select, largest=True)
    final_cand_int = cand_int[idx_selected]

    # -------------------------------
    # Step 5: Update seed map
    # -------------------------------
    new_seed = seed.clone()
    ys_new, xs_new = final_cand_int[:, 0], final_cand_int[:, 1]
    valid_update = (ys_new < H) & (xs_new < W) & (ys_new >= 0) & (xs_new >= 0)
    ys_new = ys_new[valid_update]
    xs_new = xs_new[valid_update]
    new_seed[ys_new, xs_new] = True

    return new_seed

def topk_mask(x, num, largest=True):
    """
    è¿”å›ä¸€ä¸ª maskï¼Œæ ‡è®° Tensor ä¸­æœ€å¤§çš„ num ä¸ªå…ƒç´ çš„ä½ç½®ã€‚
    
    å‚æ•°:
        x (torch.Tensor): è¾“å…¥å¼ é‡
        num (int): è¦é€‰å–çš„æœ€å¤§å…ƒç´ ä¸ªæ•°
    
    è¿”å›:
        torch.Tensor: å¸ƒå°”ç±»å‹ maskï¼Œå½¢çŠ¶ä¸ x ç›¸åŒ
    """
    if num <= 0:
        return torch.zeros_like(x, dtype=torch.bool)
    
    # å±•å¹³å¼ é‡ä»¥è¿›è¡Œæ’åº
    flat_x = x.flatten()
    
    # å¤„ç† num å¤§äºå…ƒç´ æ€»æ•°çš„æƒ…å†µ
    num = min(num, flat_x.numel())
    
    # è·å–æœ€å¤§çš„ num ä¸ªå€¼çš„ç´¢å¼•
    _, indices = torch.topk(flat_x, num, largest=largest)
    
    # åˆ›å»ºä¸ x å½¢çŠ¶ç›¸åŒçš„å…¨ False mask
    mask = torch.zeros_like(flat_x, dtype=torch.bool)
    
    # å°† top-k ç´¢å¼•ä½ç½®è®¾ä¸º True
    mask[indices] = True
    
    # æ¢å¤ mask åˆ°åŸå§‹å½¢çŠ¶
    return mask.reshape(x.shape)

def big_num_mask(x, num, largest=True):
    """
    è¿”å›ä¸€ä¸ª maskï¼Œæ ‡è®° Tensor ä¸­æœ€å¤§çš„ num ä¸ªå…ƒç´ çš„ä½ç½®ã€‚
    
    å‚æ•°:
        x (torch.Tensor): è¾“å…¥å¼ é‡
        num (int): è¦é€‰å–çš„æœ€å¤§å…ƒç´ ä¸ªæ•°
    
    è¿”å›:
        torch.Tensor: å¸ƒå°”ç±»å‹ maskï¼Œå½¢çŠ¶ä¸ x ç›¸åŒ
    """
    if num <= 0:
        return torch.zeros_like(x, dtype=torch.bool)
    
    # å±•å¹³å¼ é‡ä»¥è¿›è¡Œæ’åº
    flat_x = x.flatten()
    
    # å¤„ç† num å¤§äºå…ƒç´ æ€»æ•°çš„æƒ…å†µ
    num = min(num, flat_x.numel())

    # num = int(num * 0.8)
    
    # è·å–æœ€å¤§çš„ num ä¸ªå€¼çš„ç´¢å¼•
    vals, indices = torch.topk(flat_x, num, largest=largest)
    if largest:
        # vals = vals.min() / 2.5
        # vals = 0 if vals < 0 else vals
        result = x >= vals.min()
    else:
        # vals = vals.max() / 2.5
        # vals = 0 if vals > 0 else vals
        result = x <= vals.max()
    
    return result 

def erode_mask_4connectivity(mask, d=1):
    """
    å¯¹ mask è¿›è¡ŒåŸºäº 4-é‚»åŸŸçš„è…èš€ï¼š
    ä»…å½“åƒç´ è‡ªèº«ä¸º1ï¼Œä¸”å…¶ä¸Šã€ä¸‹ã€å·¦ã€å³å‡ä¸º1æ—¶ï¼Œæ‰ä¿ç•™ä¸º1ã€‚
    ä½¿ç”¨å·ç§¯å®ç°ï¼Œç»“æ„å…ƒä¸ºåå­—å½¢ï¼ˆ+ï¼‰ï¼Œå¤§å°ç”± d æ§åˆ¶ã€‚
    
    Args:
        mask (Tensor): [H, W], dtype=torch.float, å€¼ä¸º0æˆ–1
        d (int): é‚»åŸŸåŠå¾„ï¼Œæ§åˆ¶åå­—è‡‚é•¿ã€‚d=1 è¡¨ç¤ºç´§é‚»çš„ä¸Šä¸‹å·¦å³
    
    Returns:
        eroded (Tensor): [H, W], è…èš€åçš„ mask
    """
    H, W = mask.shape
    device = mask.device

    # åˆ›å»ºåå­—å½¢ç»“æ„å…ƒ: ä¸­å¿ƒè¡Œå’Œä¸­å¿ƒåˆ—å…¨1
    kernel_size = 2 * d + 1
    weight = torch.zeros((1, 1, kernel_size, kernel_size), device=device)
    center = d
    weight[0, 0, :, center] = 1  # å‚ç›´æ–¹å‘ï¼ˆä¸Šä¸‹ï¼‰
    weight[0, 0, center, :] = 1  # æ°´å¹³æ–¹å‘ï¼ˆå·¦å³ï¼‰

    # æ€»å…±æ¿€æ´»çš„å…ƒç´ ä¸ªæ•°ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦å…¨ä¸º1ï¼‰
    expected_sum = kernel_size * 2 - 1  # è¡Œ + åˆ— - é‡å¤çš„ä¸­å¿ƒ

    mask_float = mask.float().unsqueeze(0).unsqueeze(0)  # [1,1,H,W]

    # å·ç§¯æ“ä½œ
    conv_result = F.conv2d(mask_float, weight, padding=d)

    # åªæœ‰å½“å·ç§¯ç»“æœç­‰äº expected_sum ä¸”ä¸­å¿ƒåƒç´ ä¹Ÿä¸º1æ—¶ï¼Œæ‰ä¿ç•™
    center_mask = mask  # ä¸­å¿ƒå¿…é¡»ä¸º1
    valid = (conv_result.squeeze() == expected_sum).float()
    eroded = valid * center_mask

    return eroded

def add_uniform_points_v2(mask, seed, num1):
    credit_mask = erode_mask(mask, 1)
    target_mask = credit_mask * ~seed
    if target_mask.sum() > 0:
        return add_uniform_points_cuda(credit_mask, seed, num1)
    credit_mask = erode_mask_4connectivity(mask, 1)
    target_mask = credit_mask * ~seed
    if target_mask.sum() > 0:
        return add_uniform_points_cuda(credit_mask, seed, num1)
    return add_uniform_points_cuda(mask, seed, num1)

def add_uniform_points_v3(logits, mask, seed, num1, mode):
    add_uniform_points_cuda = add_uniform_points_grid_cuda
    if mode == "fg":
        credit_mask = mask * (logits > 0.8)
        target_mask = credit_mask * ~seed
        if target_mask.sum() > 0:
            return add_uniform_points_cuda(credit_mask, seed, num1)
        credit_mask = mask * (logits > 0.5)
        target_mask = credit_mask * ~seed
        if target_mask.sum() > 0:
            return add_uniform_points_cuda(credit_mask, seed, num1)
        credit_mask = mask * (logits > 0.1)
        target_mask = credit_mask * ~seed
        if target_mask.sum() > 0:
            return add_uniform_points_cuda(credit_mask, seed, num1)
        return add_uniform_points_cuda(mask, seed, num1)
    else:
        credit_mask = mask * (logits < 0.02)
        target_mask = credit_mask * ~seed
        if target_mask.sum() > 0:
            return add_uniform_points_cuda(credit_mask, seed, num1)
        credit_mask = mask * (logits < 0.05)
        target_mask = credit_mask * ~seed
        if target_mask.sum() > 0:
            return add_uniform_points_cuda(credit_mask, seed, num1)
        credit_mask = mask * (logits < 0.1)
        target_mask = credit_mask * ~seed
        if target_mask.sum() > 0:
            return add_uniform_points_cuda(credit_mask, seed, num1)
        return add_uniform_points_cuda(mask, seed, num1)

def get_min_value_outermost_mask(tensor):
    """
    è¾“å…¥: tensor, shape [H, W]
    è¾“å‡º: mask, shape [H, W], ç±»å‹ torch.bool æˆ– torch.float
          åªæœ‰ä¸€ä¸ªä½ç½®ä¸º 1ï¼Œå¯¹åº”æœ€å°å€¼ä¸­æœ€é è¿‘è¾¹ç•Œçš„é‚£ä¸ªç‚¹ã€‚
    """
    H, W = tensor.shape
    
    # æ­¥éª¤1: æ‰¾åˆ°æœ€å°å€¼
    min_value = tensor.min()
    
    # æ­¥éª¤2: æ‰¾åˆ°æ‰€æœ‰ç­‰äºæœ€å°å€¼çš„ä½ç½®
    min_positions = (tensor == min_value)  # bool mask
    coords = torch.nonzero(min_positions)  # [num_min, 2], æ¯è¡Œæ˜¯ (i, j)
    
    if coords.numel() == 0:
        raise ValueError("No minimum value found")

    # æ­¥éª¤3: è®¡ç®—æ¯ä¸ªæœ€å°å€¼ä½ç½®åˆ°å›¾åƒè¾¹ç•Œçš„è·ç¦»
    i_coords = coords[:, 0]  # shape: [num_min]
    j_coords = coords[:, 1]  # shape: [num_min]
    
    # åˆ°å››ä¸ªè¾¹çš„è·ç¦»ï¼šä¸Šã€ä¸‹ã€å·¦ã€å³
    dist_to_boundary = torch.stack([
        i_coords,           # åˆ°ä¸Šè¾¹
        H - 1 - i_coords,   # åˆ°ä¸‹è¾¹
        j_coords,           # åˆ°å·¦è¾¹
        W - 1 - j_coords    # åˆ°å³è¾¹
    ], dim=1)  # shape: [num_min, 4]

    # æ¯ä¸ªç‚¹çš„æœ€å°è·ç¦»ï¼ˆè¶Šå°è¶Šé å¤–ï¼‰
    min_dist = dist_to_boundary.min(dim=1).values  # shape: [num_min]

    # æ­¥éª¤4: æ‰¾åˆ°æœ€å°è·ç¦»ä¸­çš„æœ€å°å€¼ï¼ˆæœ€å¤–å›´ï¼‰ï¼Œå¦‚æœæœ‰å¤šä¸ªï¼Œå–ç¬¬ä¸€ä¸ª
    outermost_idx = min_dist.argmin()  # ç¬¬ä¸€ä¸ªæœ€å¤–å›´çš„ç´¢å¼•
    selected_coord = coords[outermost_idx]  # [2], (i, j)

    # æ­¥éª¤5: æ„é€  mask
    mask = torch.zeros(H, W, dtype=torch.bool, device=tensor.device)
    mask[selected_coord[0], selected_coord[1]] = True

    return mask

def periodic_function(t, period, amplitude, phase=0.0):
    """
    è®¡ç®—å‘¨æœŸå‡½æ•°å€¼ï¼šf(t) = a * wave((2Ï€ / p) * t + Ï†)

    Args:
        t (float or torch.Tensor): å½“å‰æ—¶é—´
        period (float): å‘¨æœŸ p
        amplitude (float): æŒ¯å¹… a
        phase (float): ç›¸ä½åç§»ï¼ˆå¼§åº¦ï¼‰ï¼Œé»˜è®¤0

    Returns:
        float or torch.Tensor: å‘¨æœŸå‡½æ•°å€¼
    """
    # è§’é¢‘ç‡
    omega = 2 * math.pi / period
    x = omega * t + phase  # ç›¸ä½

    return amplitude * (torch.sin(x) + 1) / 2

def bilateral_smooth_logits(logits, image, sigma_spatial=5.0, sigma_value=0.1):
    """
    ä½¿ç”¨ image ä½œä¸ºå¼•å¯¼å›¾ï¼Œå¯¹ logits è¿›è¡ŒåŒè¾¹å¹³æ»‘ã€‚
    æƒé‡ = ç©ºé—´é«˜æ–¯æƒé‡ * å€¼åŸŸé«˜æ–¯æƒé‡

    Args:
        logits: [H, W] çš„ tensor
        image: [H, W] çš„ tensorï¼Œä½œä¸ºå¼•å¯¼å›¾
        sigma_spatial: ç©ºé—´è·ç¦»çš„æ ‡å‡†å·®ï¼ˆæ§åˆ¶å¤šè¿œçš„åƒç´ å‚ä¸åŠ æƒï¼‰
        sigma_value: åƒç´ å€¼å·®å¼‚çš„æ ‡å‡†å·®ï¼ˆæ§åˆ¶å¤šç›¸ä¼¼çš„åƒç´ å€¼æ‰å‚ä¸åŠ æƒï¼‰

    Returns:
        smoothed_logits: [H, W] å¹³æ»‘åçš„ logits
    """
    H, W = logits.shape
    device = logits.device

    # åˆ›å»ºç©ºé—´åæ ‡ç½‘æ ¼
    y_coords, x_coords = torch.meshgrid(
        torch.arange(H, device=device),
        torch.arange(W, device=device),
        indexing='ij'
    )  # [H, W]

    # å±•å¹³æ‰€æœ‰åæ ‡å’Œåƒç´ å€¼
    y_flat = y_coords.reshape(-1)  # [H*W]
    x_flat = x_coords.reshape(-1)  # [H*W]
    image_flat = image.reshape(-1)  # [H*W]
    logits_flat = logits.reshape(-1)  # [H*W]

    # è®¡ç®—ç©ºé—´è·ç¦»çŸ©é˜µ (H*W, H*W)
    dy = y_flat.unsqueeze(1) - y_flat.unsqueeze(0)  # [H*W, H*W]
    dx = x_flat.unsqueeze(1) - x_flat.unsqueeze(0)
    spatial_dist_sq = dx**2 + dy**2  # [H*W, H*W]

    # è®¡ç®—å€¼åŸŸå·®å¼‚çŸ©é˜µ
    value_diff = image_flat.unsqueeze(1) - image_flat.unsqueeze(0)  # [H*W, H*W]
    value_diff_sq = value_diff ** 2

    # è®¡ç®—åŒè¾¹æƒé‡
    spatial_weight = torch.exp(-spatial_dist_sq / (2 * sigma_spatial**2))
    value_weight = torch.exp(-value_diff_sq / (2 * sigma_value**2))
    bilateral_weight = spatial_weight * value_weight  # [H*W, H*W]

    # å½’ä¸€åŒ–æƒé‡ï¼ˆæ¯è¡Œå’Œä¸º1ï¼‰
    weight_sum = bilateral_weight.sum(dim=1, keepdim=True)  # [H*W, 1]
    bilateral_weight = bilateral_weight / (weight_sum + 1e-8)  # é¿å…é™¤é›¶

    # åŠ æƒå¹³å‡ logits
    smoothed_logits_flat = torch.matmul(bilateral_weight, logits_flat.unsqueeze(1)).squeeze(1)  # [H*W]

    # æ¢å¤å½¢çŠ¶
    smoothed_logits = smoothed_logits_flat.view(H, W)

    return smoothed_logits

def check_cube(mask: torch.Tensor):
    """
    åœ¨äºŒå€¼ mask ä¸­æŸ¥æ‰¾æ˜¯å¦å­˜åœ¨å­¤ç«‹çš„ 3x3 å…¨ 1 çŸ©å½¢åŒºåŸŸã€‚
    â€œå­¤ç«‹â€å®šä¹‰ï¼šè¯¥ 3x3 åŒºåŸŸå‘¨å›´ä¸€åœˆï¼ˆå¤–æ‰©1åƒç´ ï¼‰å¿…é¡»å…¨ä¸º 0ï¼ˆæˆ–è¶Šç•Œï¼‰ã€‚
    
    å‚æ•°:
        mask (torch.Tensor): å½¢çŠ¶ä¸º [H, W] çš„äºŒå€¼å¼ é‡ï¼ˆå€¼ä¸º 0 æˆ– 1ï¼‰
    
    è¿”å›:
        torch.Tensor: å¦‚æœæ‰¾åˆ°ï¼Œè¿”å›è¯¥ 3x3 åŒºåŸŸï¼›
                     å¦‚æœæœªæ‰¾åˆ°ï¼Œè¿”å› None
    """
    assert mask.ndim == 2, "mask must be 2D tensor [H, W]"
    H, W = mask.shape

    if H < 3 or W < 3:
        return None

    # Step 1: æ‰¾å‡ºæ‰€æœ‰ 3x3 å…¨1åŒºåŸŸï¼ˆå·ç§¯å€¼=9ï¼‰
    kernel_3x3 = torch.ones((1, 1, 3, 3), dtype=torch.float32, device=mask.device)
    mask_f = mask.unsqueeze(0).unsqueeze(0).float()  # [1, 1, H, W]
    conv_3x3 = F.conv2d(mask_f, kernel_3x3, stride=1, padding=1)  # [1, 1, H, W]
    candidates = (conv_3x3.squeeze() == 9.0)  # [H, W]

    if not candidates.any():
        return None

    # Step 2: æ„é€  5x5 å¤–åœˆæ£€æµ‹æ ¸ï¼ˆåªæ£€æµ‹å¤–åœˆï¼Œä¸­å¿ƒ3x3ä¸º0ï¼‰
    # åˆ›å»º 5x5 æ ¸ï¼šå¤–åœˆä¸º1ï¼Œå†…3x3ä¸º0
    kernel_5x5_outer = torch.ones((1, 1, 5, 5), dtype=torch.float32, device=mask.device)
    kernel_5x5_outer[0, 0, 1:4, 1:4] = 0  # ä¸­å¿ƒ3x3ç½®0

    # å¯¹åŸå›¾åš 5x5 å·ç§¯ï¼ˆæ£€æµ‹æ¯ä¸ª 5x5 åŒºåŸŸå¤–åœˆæ˜¯å¦æœ‰1ï¼‰
    conv_5x5_outer = F.conv2d(mask_f, kernel_5x5_outer, stride=1, padding=2)  # [1, 1, H, W]
    outer1_sum = conv_5x5_outer.squeeze()  # [H-4, W-4]

    # Step 3: æ„é€  7x7 å¤–åœˆæ£€æµ‹æ ¸ï¼ˆåªæ£€æµ‹å¤–åœˆï¼Œä¸­å¿ƒ5x5ä¸º0ï¼‰
    # åˆ›å»º 7x7 æ ¸ï¼šå¤–åœˆä¸º1ï¼Œå†…5x5ä¸º0
    kernel_7x7_outer = torch.ones((1, 1, 7, 7), dtype=torch.float32, device=mask.device)
    kernel_7x7_outer[0, 0, 1:6, 1:6] = 0  # ä¸­å¿ƒ5x5ç½®0

    # å¯¹åŸå›¾åš 7x7 å·ç§¯ï¼ˆæ£€æµ‹æ¯ä¸ª 7x7 åŒºåŸŸå¤–åœˆæ˜¯å¦æœ‰1ï¼‰
    conv_7x7_outer = F.conv2d(mask_f, kernel_7x7_outer, stride=1, padding=3)  # [1, 1, H, W]
    outer2_sum = conv_7x7_outer.squeeze()  # [H-4, W-4]

    isolated = candidates & (outer1_sum <= 12) & (outer2_sum <= 1)

    return isolated

